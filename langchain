from langchain.document_loaders.csv_loader import CSVLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import VertexAIEmbeddings#deprecated
from langchain_google_vertexai import VertexAIEmbeddings#updated
from langchain.vectorstores import Pinecone
from langchain.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
import sys
import pandas as pd
a=pd.read_csv('transc_sum_v1.csv')
a.head(2)
loader=CSVLoader(file_path='only_trans.csv',encoding='utf-8',csv_args={'delimiter':','})
data=loader.load()
Request_per_min=560
embeddings=VertexAIEmbeddings(requests_per_minute=Request_per_min)
DB_FAISS='VectorStore/db_faiss_3'
docsearch=FAISS.from_documents(text_chunks,embeddings)
docsearch.save_local(DB_FAISS)
from langchain.llms import VertexAI
llm=VertexAI(model_name='text-bison-32k',maximum_output_tokens=8000,tempearature=0.2,top_p=0.8,top_k=40,verbose=True)

import sys
qa = ConversationalRetrievalChain.from_llm(llm, retriever=docsearch.as_retriever())
while True:
  chat_history = []
  query = input(f"Input Prompt: ")
  if query == 'exit':
    print('Exiting')
    sys.exit()
  if query == '':
    continue
  result = qa({"question":query, "chat_history":chat_history})
  print("Response: ", result['answer'])
This is my code execution has no problem but the problem  is the output. Ill show it below
Input Prompt:  Generated top 10 recurring issues discussed in the calls and the count and also give me the fixes given on the that call if not means give not solved
Response:   **Top 10 Recurring Issues:**
1. **ONT/ONU not linking/registering with OLT (12 instances):**
   - **Fixes:**
     - Verify LCP and ensure it's in the right place.
     - Swap the ONT.
     - Check for outages and add a note to the outage ticket if necessary.
2. **No internet connectivity/No ping (5 instances):**
   - **Fixes:**
     - Reset the ONT and DPU.
     - Check operational state and ensure it's not down.
     - Create a
See the output is not fully completed the llm has working good it has potential problem is in retrieval chain



RUN_DT = []
primary_order_item = []
ECO_SOURCE = []
Product_Category = []
net_mrr = []
order_status = []
CUST_ORDER_NBR = []
SERV_ORDER_NBR  = []
SOU  = []
SERVICE_ID = []
TASK_NAME = []
TASK_STATUS = []
START_DATE = []
END_DATE = []
CUST_ORDER_SOURCE = []
DW_SOURCE_SYSTEM = []
MODIFIED_DT = []
PRODUCT_NAME = []
SERV_ORDER_TYP = []
SERV_ORDER_ACTION_TYP = []
SERV_ORDER_STATUS = []
CUST_COMMIT_DT = []
PRODUCT = []
CUST_REQST_DT = []
TASK_CREATE_DT = []
A_COUNTRY_NAME = []
Z_COUNTRY_NAME = []
TASK_DUE_DT = []
IS_NEW_VERSION = []
SM_VERSION = []
# M_VERSION = []
IS_FALLOUT_TASK = []
ONNET=[]
total_tasks = []
total_products = []
"Total_Tasks","total_products","total_fallouts",
ONNET_COUNT=[]
for order_id in df['primary_order_item'].unique():
    sub_df = df[df['primary_order_item'] == order_id]
    sub_df = sub_df.sort_values(by="START_DATE",ascending=True)
    total_tasks.append(len(sub_df))
    total_products.append(sub_df['PRODUCT_NAME'].nunique())
    RUN_DT.append(sub_df['RUN_DT'].iloc[0])
    primary_order_item.append(sub_df['primary_order_item'].iloc[0])
    ECO_SOURCE.append(sub_df['ECO_SOURCE'].iloc[0])
    Product_Category.append(sub_df['Product_Category'].iloc[0])
    net_mrr.append(sub_df['net_mrr'].iloc[0])
    order_status.append(sub_df['order_status'].iloc[0])
    CUST_ORDER_NBR.append(sub_df['CUST_ORDER_NBR'].iloc[0])
    SERV_ORDER_NBR.append(sub_df['SERV_ORDER_NBR'].iloc[0]) 
    SOU.append(sub_df['SOU'].iloc[0])
    SERVICE_ID.append(sub_df['SERVICE_ID'].iloc[0])
    TASK_NAME.append(len(sub_df['TASK_NAME'].unique()))
    try:
        TASK_STATUS.append((dict(sub_df['TASK_STATUS'].value_counts())['Completed']/len(sub_df))*100)
    except:
        TASK_STATUS.append(0)
    START_DATE.append(sub_df['START_DATE'].iloc[0])
    END_DATE.append(sub_df['END_DATE'].iloc[0])
    CUST_ORDER_SOURCE.append(sub_df['CUST_ORDER_SOURCE'].iloc[0])
    DW_SOURCE_SYSTEM.append(sub_df['DW_SOURCE_SYSTEM'].iloc[0])
    MODIFIED_DT.append(sub_df['MODIFIED_DT'].iloc[0])
    prd = ','.join(sub_df['PRODUCT_NAME'].dropna().unique())
    PRODUCT_NAME.append(prd)
    
    SERV_ORDER_TYP.append(sub_df['SERV_ORDER_TYP'].iloc[0])
    SERV_ORDER_ACTION_TYP.append(sub_df['SERV_ORDER_ACTION_TYP'].iloc[0])
    SERV_ORDER_STATUS.append(sub_df['SERV_ORDER_STATUS'].iloc[0])
    CUST_COMMIT_DT.append(sub_df['CUST_COMMIT_DT'].iloc[0])
    PRODUCT.append(sub_df['PRODUCT'].iloc[0])
    CUST_REQST_DT.append(sub_df['CUST_REQST_DT'].iloc[0])
    TASK_CREATE_DT.append(sub_df['TASK_CREATE_DT'].iloc[0])
    A_COUNTRY_NAME.append(sub_df['A_COUNTRY_NAME'].iloc[0])
    Z_COUNTRY_NAME.append(sub_df['Z_COUNTRY_NAME'].iloc[0])
    TASK_DUE_DT.append(sub_df['TASK_DUE_DT'].iloc[-1])
    IS_NEW_VERSION.append(sub_df['IS_NEW_VERSION'].iloc[0])
    SM_VERSION.append(sub_df['SM_VERSION'].iloc[0])
    ONNET.append(sub_df['onnet'].iloc[0])
    ONNET_COUNT.append(sub_df['onnet_count'].iloc[0])
    # M_VERSION.append(sub_df['M_VERSION'].iloc[0])
    try:
        IS_FALLOUT_TASK.append((sub_df['IS_FALLOUT_TASK'].value_counts()[1]/len(sub_df))*100)
    except KeyError:
        IS_FALLOUT_TASK.append(0)

new_df = pd.DataFrame({"RUN_DT":RUN_DT,
"primary_order_item":primary_order_item,
'ECO_SOURCE':ECO_SOURCE,
'Product_Category':Product_Category,
'net_mrr':net_mrr,
'order_status':order_status,
'CUST_ORDER_NBR':CUST_ORDER_NBR,
'SERV_ORDER_NBR':SERV_ORDER_NBR ,
'SOU':SOU, 
'SERVICE_ID':SERVICE_ID,
'TASK_NAME':TASK_NAME,
'TASK_STATUS':TASK_STATUS,
'START_DATE':START_DATE,
'END_DATE':END_DATE,
'CUST_ORDER_SOURCE':CUST_ORDER_SOURCE,
'DW_SOURCE_SYSTEM':DW_SOURCE_SYSTEM,
'MODIFIED_DT':MODIFIED_DT,
'PRODUCT_NAME':PRODUCT_NAME,
'SERV_ORDER_TYP':SERV_ORDER_TYP,
'SERV_ORDER_ACTION_TYP':SERV_ORDER_ACTION_TYP,
'SERV_ORDER_STATUS':SERV_ORDER_STATUS,
'CUST_COMMIT_DT':CUST_COMMIT_DT,
'PRODUCT':PRODUCT,
'CUST_REQST_DT':CUST_REQST_DT,
'TASK_CREATE_DT':TASK_CREATE_DT,
'A_COUNTRY_NAME':A_COUNTRY_NAME,
'Z_COUNTRY_NAME':Z_COUNTRY_NAME,
'TASK_DUE_DT':TASK_DUE_DT,
'IS_NEW_VERSION':IS_NEW_VERSION,
'SM_VERSION':SM_VERSION,
# 'M_VERSION':M_VERSION,
'onnet':ONNET,
'onnet_count':ONNET_COUNT,
'total_fallouts':IS_FALLOUT_TASK,
"total_tasks":total_tasks,
"total_products":total_products})




import pandas as pd

# Grouping by 'primary_order_item' and aggregating values
grouped = df.groupby('primary_order_item').agg({
    'RUN_DT': 'first',
    'ECO_SOURCE': 'first',
    'Product_Category': 'first',
    'net_mrr': 'first',
    'order_status': 'first',
    'CUST_ORDER_NBR': 'first',
    'SERV_ORDER_NBR': 'first',
    'SOU': 'first',
    'SERVICE_ID': 'first',
    'TASK_NAME': lambda x: x.nunique(),
    'TASK_STATUS': lambda x: (x.eq('Completed').sum() / len(x)) * 100 if len(x) > 0 else 0,
    'START_DATE': 'first',
    'END_DATE': 'first',
    'CUST_ORDER_SOURCE': 'first',
    'DW_SOURCE_SYSTEM': 'first',
    'MODIFIED_DT': 'first',
    'PRODUCT_NAME': lambda x: ','.join(x.dropna().unique()),
    'SERV_ORDER_TYP': 'first',
    'SERV_ORDER_ACTION_TYP': 'first',
    'SERV_ORDER_STATUS': 'first',
    'CUST_COMMIT_DT': 'first',
    'PRODUCT': 'first',
    'CUST_REQST_DT': 'first',
    'TASK_CREATE_DT': 'first',
    'A_COUNTRY_NAME': 'first',
    'Z_COUNTRY_NAME': 'first',
    'TASK_DUE_DT': 'last',
    'IS_NEW_VERSION': 'first',
    'SM_VERSION': 'first',
    'onnet': 'first',
    'onnet_count': 'first',
    'IS_FALLOUT_TASK': lambda x: (x.sum() / len(x)) * 100 if len(x) > 0 else 0,
    'total_tasks': 'size',
    'total_products': lambda x: x.nunique()
})

# Creating the new DataFrame
new_df = grouped.reset_index()




z = []
for i in df_6['WM_TASK_ID'].unique():
    if i in merged_df['WM_TASK_ID'].unique():
        z.append(i)
        print(i)
    else:
        pass
    
print(len(z))



# Convert unique values to sets
df6_unique_ids = set(df_6['WM_TASK_ID'].unique())
merged_unique_ids = set(merged_df['WM_TASK_ID'].unique())

# Find the intersection of unique values
z = df6_unique_ids.intersection(merged_unique_ids)

# Print the common values
for i in z:
    print(i)

print(len(z))




TOTAL_TASKS = []
TOTAL_PRODUCTS = []
TOTAL_MILESTONES = []
TOTAL_FALLOUTS = []
TASK_COMPLETION = []
for i in final_df['PRIMARY_ORDER_ITEM'].unique():
    sdf = final_df[final_df['PRIMARY_ORDER_ITEM'] == i]
    for j in sdf['PRODUCT_NAME']:
        v = sdf[sdf['PRODUCT_NAME'] == j ]
        TOTAL_FALLOUTS.append(sdf['IS_FALLOUT_TASK'].sum())
        TOTAL_PRODUCTS.append(v['PRODUCT_NAME'].count())
        TOTAL_TASKS.append(v['TASK_NAME'].count())
        TOTAL_MILESTONES.append(v['TASK MILESTONE'].count())
        tc = sum(k in ['I_COMPLETED', 'Completed ', 'Complete', 'COMPLETE'] for k in sdf['TASK_STATUS'] )
        TASK_COMPLETION.extend([tc] * len(sdf))


import pandas as pd
import numpy as np

# Assuming you have a DataFrame named 'data' with your dataset
# Replace 'dependent_variable' with the name of your dependent variable
dependent_variable = 'target_variable'

# List of columns where NaN values are considered important
important_nan_columns = ['column1', 'column2']

# Impute NaN values with a placeholder value (e.g., -999) for those columns
data_imputed = data.copy()
data_imputed[important_nan_columns] = data_imputed[important_nan_columns].fillna(-999)

# Compute the correlation matrix
correlation_matrix = data_imputed.corr()

# Get correlation of dependent variable with all other variables
correlation_with_dependent = correlation_matrix[dependent_variable]

# Threshold for correlation strength
threshold = 0.5  # Adjust as needed

# Filter variables highly correlated with the dependent variable
significant_correlation = correlation_with_dependent[abs(correlation_with_dependent) > threshold]

# Extract variable names with significant correlation
independent_variables = significant_correlation.index.tolist()
independent_variables.remove(dependent_variable)

# Split data into independent and dependent variables
X = data_imputed[independent_variables]  # Independent variables
y = data_imputed[dependent_variable]     # Dependent variable


 'BLOCK_TYPE', 'CATEGORY', 'REASON', 'HOLD_TIME',
       'DESCRIPTION', 'BLK_SEQ', 'BLK_CREATED_DTTM', 'BLK_MODIFIED_DTTM'

# Step 1: Calculate total number of tasks and order duration
df['Total Tasks'] = df['TOTAL TASKS']
df['Order Duration'] = (df['END DATE'] - df['START DATE']).dt.days

# Step 2: Normalize values
df['Normalized_Total_Tasks'] = (df['Total Tasks'] - df['Total Tasks'].min()) / (df['Total Tasks'].max() - df['Total Tasks'].min())
df['Normalized_Order_Duration'] = (df['Order Duration'] - df['Order Duration'].min()) / (df['Order Duration'].max() - df['Order Duration'].min())

# Step 3: Combine factors
weight_total_tasks = 0.5  # Weight for total tasks
weight_order_duration = 0.5  # Weight for order duration
df['Order_Activity_Intensity'] = (weight_total_tasks * df['Normalized_Total_Tasks']) + (weight_order_duration * df['Normalized_Order_Duration'])

# Step 4: Add the "Order Activity Intensity" column to the dataframe
# Optionally, you can drop the intermediate columns used for calculation
df.drop(['Total Tasks', 'Order Duration', 'Normalized_Total_Tasks', 'Normalized_Order_Duration'], axis=1, inplace=True)



training accuracy - 0.9924878094079754
test accuracy - 0.9689728041002216
mean squared error - 17.113885627530365
mean abosolute error - 1.6384412955465586
r square - 0.9629688787975816

training accuracy - 0.9776850685327265
test accuracy - 0.9768431179975037
mean squared error - 12.77280200766607
mean abosolute error - 1.1235911333529172
r square - 0.9690718871956321

training accuracy - 0.9855197375223229
test accuracy - 0.9636193595773416
mean squared error - 20.06672215113483
mean abosolute error - 1.88195645644342
r square - 0.9588830154515672



df_2.head(2)
x = df_2.drop(["O_NO_OF_DAYS_TAKEN"],axis=1)
y= df_2['O_NO_OF_DAYS_TAKEN']
x['PRODUCT_NAME']=LabelEncoder().fit_transform(x['PRODUCT_NAME'])


x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,shuffle = True)

from sklearn.linear_model import Ridge, Lasso
model_reg = Lasso(alpha=0.1).fit(x_train,y_train)
pred = model_reg.predict(x_test)
print(f"training accuracy - {model_reg.score(x_train,y_train)}")
print(f"test accuracy - {model_reg.score(x_test,y_test)}")
print(f"mean squared error - {mean_squared_error(pred,y_test)}")
print(f"mean abosolute error - {mean_absolute_error(pred,y_test)}")
print(f"r square - {r2_score(pred,y_test)}")
plt.xlabel("predicted")
plt.ylabel("actual")
plt.title("actual vs predicted")
sns.scatterplot(x=pred,y=y_test)

training accuracy - 0.8271040020605858

from sklearn.linear_model import Ridge, Lasso
model_reg_2 = joblib.load('/Workspace/Users/vibhish.ravikumar@lumen.com/time_pred_model.pkl')
pred = model_reg_2.predict(x_test)
print(f"test accuracy - {model_reg_2.score(x_test,y_test)}")
print(f"mean squared error - {mean_squared_error(pred,y_test)}")
print(f"mean abosolute error - {mean_absolute_error(pred,y_test)}")
print(f"r square - {r2_score(pred,y_test)}")
plt.xlabel("predicted")
plt.ylabel("actual")
plt.title("actual vs predicted")
sns.scatterplot(x=pred,y=y_test)
test accuracy - 0.7994369566481376
mean squared error - 280.1247967749637
mean abosolute error - 12.954296266700855
r square - 0.7380866983778717


training accuracy - 0.6466765397971537
test accuracy - 0.6735908143600162
mean squared error - 192.89347878600734
mean abosolute error - 5.0101752070117005
r square - 0.39563733336741114



query_5 = """
SELECT WM.*
FROM srvc_dlvr.dash.wm_task_data AS WM
WHERE (WM.TASK_STATUS_CODE = 300)
    AND (
        (WM.DERIVED_SOU IN (
            SELECT primary_order_item
            FROM SRVC_DLVR.DASH.VW_TASK_DATA_WIP_ORDER
        ) AND WM.SERV_ORDER_NBR IS NULL)
        OR
        (WM.SERV_ORDER_NBR IN (
            SELECT primary_order_item
            FROM SRVC_DLVR.DASH.VW_TASK_DATA_WIP_ORDER
        ) AND WM.DERIVED_SOU IS NULL)
    )
    OR
    (WM.DERIVED_SOU IN (
        SELECT primary_order_item
        FROM SRVC_DLVR.DASH.VW_TASK_DATA_WIP_ORDER
    ) AND WM.SERV_ORDER_NBR IN (
        SELECT primary_order_item
        FROM SRVC_DLVR.DASH.VW_TASK_DATA_WIP_ORDER
    ))
"""

query_6 = """select * from  [SRVC_DLVR].[DASH].[WIP_ORDER_BLOCKER]  where blk_seq=1"""

join query_5 and query_6 into single query_7 such that primary_order_item from [SRVC_DLVR].[DASH].[WIP_ORDER_BLOCKER] where blk_seq = 1 matches with the DERIVED_SOU and SERV_ORDER_NBR condition in the query_5

query_7 = """SELECT WM.*
FROM srvc_dlvr.dash.wm_task_data AS WM
INNER JOIN [SRVC_DLVR].[DASH].[WIP_ORDER_BLOCKER] AS WB ON (
    (WM.DERIVED_SOU = WB.primary_order_item AND WM.SERV_ORDER_NBR IS NULL)
    OR
    (WM.SERV_ORDER_NBR = WB.primary_order_item AND WM.DERIVED_SOU IS NULL)
    OR
    (WM.DERIVED_SOU = WB.primary_order_item AND WM.SERV_ORDER_NBR = WB.primary_order_item)
)
WHERE (WM.TASK_STATUS_CODE = 300)
AND (
    (WB.primary_order_item IN (
        SELECT primary_order_item
        FROM SRVC_DLVR.DASH.VW_TASK_DATA_WIP_ORDER
    ) AND WM.SERV_ORDER_NBR IS NULL)
    OR
    (WB.primary_order_item IN (
        SELECT primary_order_item
        FROM SRVC_DLVR.DASH.VW_TASK_DATA_WIP_ORDER
    ) AND WM.DERIVED_SOU IS NULL)
)
"""

cat = ['PRODUCT_NAME',"ECO_SOURCE","TASK_SOURCE","TASK_STATUS","SERV_ORDER_TYP","SERV_ORDER_ACTION_TYP",
                     "PAID_EXPEDITE","BLOCK_TYPE","CATEGORY","SUCC_TASK_NAME","NET_MRR_TYPE","SUCC_TASK_STATUS",
                     "SERVICE_ORDER_STATUS","MODIFIED_DTTM",'TASK_STATUS_CODE',"VENDOR_BUILD_BLOCK_STATUS"]

for i in cat:
    new_col = f"cat_{i}"
    uni = {}
    uniq = df_build[i].unique()
    for j in range(len(uniq)):
        uni[uniq[j]] = j
        df_build[new_col] = df_build[i].map(uni)

df_build_1 = df_build.drop(cat,axis=1)
df_build_1.head(n=2)



from sklearn.base import BaseEstimator, TransformerMixin

class CategoricalEncoder(BaseEstimator, TransformerMixin):
    def __init__(self, cat_columns):
        self.cat_columns = cat_columns
        self.encoding_dict = {}
    
    def fit(self, X, y=None):
        for col in self.cat_columns:
            unique_values = X[col].unique()
            encoding = {val: idx for idx, val in enumerate(unique_values)}
            self.encoding_dict[col] = encoding
        return self
    
    def transform(self, X):
        X_copy = X.copy()
        for col, encoding in self.encoding_dict.items():
            new_col_name = f"cat_{col}"
            X_copy[new_col_name] = X_copy[col].map(encoding)
            X_copy.drop(col, axis=1, inplace=True)
        return X_copy


from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier

# Assuming df_build is your DataFrame and cat is your list

# Define the pipeline
pipeline = Pipeline([
    ('categorical_encoder', CategoricalEncoder(cat)),
    ('model', RandomForestClassifier())  # Example model, replace with your model
])

# Fit the pipeline
pipeline.fit(df_build.drop('target_column', axis=1), df_build['target_column'])

# Predict using the pipeline
predictions = pipeline.predict(test_data)



import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# Assuming you have your data loaded into a DataFrame called 'data'
# Split data into features and target variable
X = data.drop('time_taken', axis=1)
y = data['time_taken']

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define a pipeline with StandardScaler and RandomForestRegressor
pipeline_rf = Pipeline([
    ('scaler', StandardScaler()),
    ('regressor', RandomForestRegressor())
])

# Define hyperparameters grid for RandomForestRegressor
param_grid_rf = {
    'regressor__n_estimators': [50, 100, 150],
    'regressor__max_depth': [None, 10, 20],
    'regressor__min_samples_split': [2, 5, 10],
    'regressor__min_samples_leaf': [1, 2, 4]
}

# Perform GridSearchCV with cross-validation for RandomForestRegressor
grid_search_rf = GridSearchCV(pipeline_rf, param_grid_rf, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search_rf.fit(X_train, y_train)

# Best hyperparameters for RandomForestRegressor
print("Best Hyperparameters for RandomForestRegressor:", grid_search_rf.best_params_)

# Use the best RandomForestRegressor model obtained from grid search
best_model_rf = grid_search_rf.best_estimator_

# Fit the best RandomForestRegressor model on the entire training data
m1 = best_model_rf.fit(x_train, y_train)

# Evaluate the best RandomForestRegressor model on the test set
y_pred_rf = best_model_rf.predict(x_test)
mse_rf = mean_squared_error(y_test, y_pred_rf)

print(f"training accuracy for RF - {m1.score(x_train,y_train)}")
print(f"test accuracy for RF - {m1.score(x_test,y_test)}")
print("Mean Squared Error on Test Set (RandomForestRegressor):", mse_rf)

# Scatter plot for RandomForestRegressor
plt.figure(figsize=(8, 6))
plt.scatter(y_pred_rf, y_test, alpha=0.5)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('RandomForestRegressor')
plt.show()


# Define a pipeline with StandardScaler and GradientBoostingRegressor
pipeline_gb = Pipeline([
    ('scaler', StandardScaler()),
    ('regressor', GradientBoostingRegressor())
])

# Define hyperparameters grid for GradientBoostingRegressor
param_grid_gb = {
    'regressor__n_estimators': [50, 100, 150],
    'regressor__learning_rate': [0.01, 0.1, 0.2],
    'regressor__max_depth': [3, 5, 7]
}

# Perform GridSearchCV with cross-validation for GradientBoostingRegressor
grid_search_gb = GridSearchCV(pipeline_gb, param_grid_gb, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search_gb.fit(X_train, y_train)

# Best hyperparameters for GradientBoostingRegressor
print("Best Hyperparameters for GradientBoostingRegressor:", grid_search_gb.best_params_)

# Use the best GradientBoostingRegressor model obtained from grid search
best_model_gb = grid_search_gb.best_estimator_

# Fit the best GradientBoostingRegressor model on the entire training data
m2 = best_model_gb.fit(x_train, y_train)

# Evaluate the best GradientBoostingRegressor model on the test set
y_pred_gb = best_model_gb.predict(x_test)
mse_gb = mean_squared_error(y_test, y_pred_gb)

print(f"training accuracy for GBR - {m2.score(x_train,y_train)}")
print(f"test accuracy for GBR - {m2.score(x_test,y_test)}")
print("Mean Squared Error on Test Set (GradientBoostingRegressor):", mse_gb)

# Scatter plot for GradientBoostingRegressor
plt.figure(figsize=(8, 6))
plt.scatter(y_pred_gb, y_test, alpha=0.5)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('GradientBoostingRegressor')
plt.show()


# Define a pipeline with StandardScaler and SVR
pipeline_svr = Pipeline([
    ('scaler', StandardScaler()),
    ('regressor', SVR())
])

# Define hyperparameters grid for SVR
param_grid_svr = {
    'regressor__C': [0.1, 1, 10],
    'regressor__epsilon': [0.01, 0.1, 1],
    'regressor__kernel': ['linear', 'rbf']
}

# Perform GridSearchCV with cross-validation for SVR
grid_search_svr = GridSearchCV(pipeline_svr, param_grid_svr, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search_svr.fit(X_train, y_train)

# Best hyperparameters for SVR
print("Best Hyperparameters for SVR:", grid_search_svr.best_params_)

# Use the best SVR model obtained from grid search
best_model_svr = grid_search_svr.best_estimator_

# Fit the best SVR model on the entire training data
m3 = best_model_svr.fit(x_train, y_train)

# Evaluate the best SVR model on the test set
y_pred_svr = best_model_svr.predict(x_test)
mse_svr = mean_squared_error(y_test, y_pred_svr)

print(f"training accuracy for SVR - {m3.score(x_train,y_train)}")
print(f"test accuracy for SVR - {m3.score(x_test,y_test)}")
print("Mean Squared Error on Test Set (SVR):", mse_svr)

# Scatter plot for SVR
plt.figure(figsize=(8, 6))
plt.scatter(y_pred_svr, y_test, alpha=0.5)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('SVR')
plt.show()

# Define a pipeline with StandardScaler and LinearRegression
pipeline_lr = Pipeline([
    ('scaler', StandardScaler()),
    ('regressor', LinearRegression())
])

# No hyperparameters for LinearRegression

# Fit the LinearRegression model on the entire training data
m4 = pipeline_lr.fit(x_train, y_train)

# Evaluate the LinearRegression model on the test set
y_pred_lr = pipeline_lr.predict(X_test)
mse_lr = mean_squared_error(y_test, y_pred_lr)

print(f"training accuracy for LR - {m4.score(x_train,y_train)}")
print(f"test accuracy for LR - {m4.score(x_test,y_test)}")
print("Mean Squared Error on Test Set (LinearRegression):", mse_lr)

# Scatter plot for LinearRegression
plt.figure(figsize=(8, 6))
plt.scatter(y_pred_lr, y_test, alpha=0.5)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('LinearRegression')
plt.show()


# Define a pipeline with StandardScaler and Lasso
pipeline_lasso = Pipeline([
    ('scaler', StandardScaler()),
    ('regressor', Lasso(alpha=0.1))
])

# No hyperparameters for Lasso

# Fit the Lasso model on the entire training data
m5 = pipeline_lasso.fit(x_train, y_train)

# Evaluate the Lasso model on the test set
y_pred_lasso = pipeline_lasso.predict(x_test)
mse_lasso = mean_squared_error(y_test, y_pred_lasso)

print(f"training accuracy for Lasso - {m5.score(x_train,y_train)}")
print(f"test accuracy for Lasso - {m5.score(x_test,y_test)}")
print("Mean Squared Error on Test Set (Lasso):", mse_lasso)

# Scatter plot for Lasso
plt.figure(figsize=(8, 6))
plt.scatter(y_pred_lasso, y_test, alpha=0.5)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Lasso')
plt.show()

# Define a pipeline with StandardScaler and XGBRegressor
pipeline_xgb = Pipeline([
    ('scaler', StandardScaler()),
    ('regressor', xgb.XGBRegressor())
])

# Define hyperparameters grid for XGBRegressor
param_grid_xgb = {
    'regressor__learning_rate': [0.01, 0.1, 0.2],
    'regressor__max_depth': [3, 5, 7],
    'regressor__n_estimators': [50, 100, 150]
}

# Perform GridSearchCV with cross-validation for XGBRegressor
grid_search_xgb = GridSearchCV(pipeline_xgb, param_grid_xgb, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)
grid_search_xgb.fit(X_train, y_train)

# Best hyperparameters for XGBRegressor
print("Best Hyperparameters for XGBRegressor:", grid_search_xgb.best_params_)

# Use the best XGBRegressor model obtained from grid search
best_model_xgb = grid_search_xgb.best_estimator_

# Fit the best XGBRegressor model on the entire training data
best_model_xgb.fit(X_train, y_train)

# Evaluate the best XGBRegressor model on the test set
y_pred_xgb = best_model_xgb.predict(X_test)
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
print("Mean Squared Error on Test Set (XGBRegressor):", mse_xgb)

# Scatter plot for XGBRegressor
plt.figure(figsize=(8, 6))
plt.scatter(y_pred_xgb, y_test, alpha=0.5)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('XGBRegressor')
plt.show()





# Connection details
jdbcHostname = "asfgdfsv"
jdbcPort = 1233
jdbcDatabase = "DFAGZE"
tp_user = dbutils.secrets.get("sdaiml", "tp-user")
tp_pw = dbutils.secrets.get("sdaiml", "tp-pass")
jdbcDriver = "com.microsoft.sqlserver.jdbc.SQLServerDriver"
jdbcUrl = f"jdbc:sqlserver://{jdbcHostname}:{jdbcPort};databaseName={jdbcDatabase};user={tp_user};password={tp_pw};encrypt=true;trustServerCertificate=true"


# Build query
query_1 = """
SELECT WIP.*,
    CASE
        WHEN MD_counts.onnet_count > 0 THEN 'Yes' ELSE 'No'
    END as 'onnet',COALESCE(MD_counts.onnet_count, 0) as onnet_count
FROM
    (SELECT *, COUNT(*) OVER(PARTITION BY primary_order_item) as total_count
    FROM [DASH].[VW_TASK_DATA_WIP_ORDER]
    WHERE START_DATE >= GETDATE() - 250 AND order_status = 'Installed' AND Product_Category ='IP') as WIP
INNER JOIN
    (SELECT SRVC_ORDER_UNIT, COUNT(*) as onnet_count
    FROM [DASH].MEASURED_DATA
    WHERE ACCESS_METHOD = 'On-Net'
    GROUP BY SRVC_ORDER_UNIT) as MD_counts
ON
    WIP.primary_order_item = MD_counts.SRVC_ORDER_UNIT
"""

# Submit query and load results into df
spark_df_1 = (
    spark.read.format("jdbc")
    .option("url", jdbcUrl)
    .option("dbtable", f"({query_1}) as t")
    .option("user", tp_user)
    .option("password", tp_pw)
    .option("driver", jdbcDriver)
    .load()
)

# optional convert spark df -> pandas df
df_1 = spark_df_1.toPandas()

conn = pymssql.connect(
    server="USIDCVSQL0255.ctl.intranet",
    user="sd_aiml_tp",
    password="nM2H3d~7",
    database="SRVC_DLVR",
)
cursor = conn.cursor()
query_2 = """SELECT * FROM [DASH].MEASURED_DATA"""

df_2 = pd.read_sql_query(query_2, conn)

query_5 = """
SELECT WM.*
FROM srvc_dlvr.dash.wm_task_data AS WM
WHERE (WM.TASK_STATUS_CODE = 300)
    AND (
        (WM.DERIVED_SOU IN (
            SELECT primary_order_item
            FROM SRVC_DLVR.DASH.VW_TASK_DATA_WIP_ORDER
        ) AND WM.SERV_ORDER_NBR IS NULL)
        OR
        (WM.SERV_ORDER_NBR IN (
            SELECT primary_order_item
            FROM SRVC_DLVR.DASH.VW_TASK_DATA_WIP_ORDER
        ) AND WM.DERIVED_SOU IS NULL)
    )
    OR
    (WM.DERIVED_SOU IN (
        SELECT primary_order_item
        FROM SRVC_DLVR.DASH.VW_TASK_DATA_WIP_ORDER
    ) AND WM.SERV_ORDER_NBR IN (
        SELECT primary_order_item
        FROM SRVC_DLVR.DASH.VW_TASK_DATA_WIP_ORDER
    ))
"""


# Submit query and load results into df
spark_df_5 = (
    spark.read.format("jdbc")
    .option("url", jdbcUrl)
    .option("dbtable", f"({query_5}) as t")
    .option("user", tp_user)
    .option("password", tp_pw)
    .option("driver", jdbcDriver)
    .load()
)

# optional convert spark df -> pandas df
df_5 = spark_df_5.toPandas()
df_5 = df_5[df_5["TASK_STATUS_CODE"] == "300"]

query_6 = """select * from  [SRVC_DLVR].[DASH].[WIP_ORDER_BLOCKER]  where blk_seq=1 and TASK_STATUS	= 'Blocked' """
# Submit query and load results into df
spark_df_6 = (
    spark.read.format("jdbc")
    .option("url", jdbcUrl)
    .option("dbtable", f"({query_6}) as t")
    .option("user", tp_user)
    .option("password", tp_pw)
    .option("driver", jdbcDriver)
    .load()
)

# optional convert spark df -> pandas df
df_6 = spark_df_6.toPandas()

subset_columns = [
    "SRVC_ORDER_UNIT",
    "BU_SHORT",
    "BU_SEGMENT",
    "SD_BU",
    "Sales_Channel",
    "SOURCE_SYSTEM",
    "NET_IND",
    "ORDER_ACTIVITY",
    "ORDER_TYPE",
    "MRR",
    "PRODUCT_CATEGORY",
    "GL_PRODUCT_FAMILY_NAME",
    "PROD_TYPE",
    "GL_PRODUCT_NAME",
    "PROV_PRODUCT",
    "MANAGED_SRVCS_FLAG",
    "PRODUCT_SUBTYP_NAME",
    "CUSTOMER_SIGN_DT",
    "ORIG_SA_RECD_DT",
    "ORIG_CCD_SET_DT",
    "ORIG_CUST_REQUEST_DT",
    "CUST_REQUEST_DT",
    "ORIG_CUST_COMMIT_DT",
    "CUST_COMMIT_DT",
    "INSTALL_DT",
    "INSTALL_MO",
    "A_CITY_NAME",
    "A_STATE_NAME",
    "Z_CITY_NAME",
    "Z_STATE_NAME",
    "PAID_EXPEDITE",
    "NET_MRR_TYPE",
    "ORDER_SOURCE_SYSTEM_NAME",
    "ACCESS_METHOD",
    "VENDOR_BUILD_BLOCK_STATUS",
]
measured_df = df_2[subset_columns]
ip_measured_df = measured_df[measured_df["PRODUCT_CATEGORY"] == "IP"]

ip_measured_df.rename(columns={"SRVC_ORDER_UNIT": "PRIMARY_ORDER_ITEM"}, inplace=True)
ip_measured_df.columns = ip_measured_df.columns.str.upper()
df_1.columns = df_1.columns.str.upper()
df_6.columns = df_6.columns.str.upper()
df_raw = df_1.merge(ip_measured_df, on="PRIMARY_ORDER_ITEM", how="left")
df_5.rename(columns={"DERIVED_SOU": "PRIMARY_ORDER_ITEM"}, inplace=True)
df_7 = df_6.merge(df_5, on=["PRIMARY_ORDER_ITEM", "TASK_NAME"], how="left")
df_7 = df_7.drop_duplicates(subset=["PRIMARY_ORDER_ITEM", "TASK_NAME"], keep="last")
df_7["TASK_STATUS"] = df_7["TASK_STATUS_y"].combine_first(df_7["TASK_STATUS_x"])
df_7.drop(["TASK_STATUS_x", "TASK_STATUS_y"], axis=1, inplace=True)
merge_df = pd.merge(
    df_raw,
    df_7,
    on=[
        "PRIMARY_ORDER_ITEM",
        "TASK_NAME",
        "TASK_STATUS",
        "SERV_ORDER_NBR",
        "ORDER_TYPE",
    ],
    how="left",
)

HOLD_TIME = date the block should be removed/updated for that particular block




import pandas as pd
import numpy as np
from sklearn.feature_selection import SelectKBest, f_classif
from scipy.stats import pearsonr

# Load the dataset
telecom_df = pd.read_csv('telecom_data.csv')

# Assuming the target variable is named 'churn' (1 for churned, 0 for not churned)
X = telecom_df.drop('churn', axis=1)  # Features
y = telecom_df['churn']  # Target

# Identify datetime features
datetime_features = X.select_dtypes(include=['datetime64']).columns.tolist()

# Drop datetime features for feature selection (can be handled differently based on the use case)
X_for_selection = X.drop(datetime_features, axis=1)

# Define the number of features to select
k_best_features = 5

# 1. Statistical Methods: SelectKBest
selector = SelectKBest(score_func=f_classif, k=k_best_features)
selector.fit(X_for_selection, y)

# Get the indices of the selected features
selected_features_indices = selector.get_support(indices=True)

# Get the names of the selected features
selected_features_skbest = X_for_selection.columns[selected_features_indices]

# 2. Correlation Methods: Pearson Correlation
correlation_scores = []
for column in X_for_selection.columns:
    correlation_score = pearsonr(X_for_selection[column], y)[0]
    correlation_scores.append((column, correlation_score))

# Sort features based on correlation score
sorted_features = sorted(correlation_scores, key=lambda x: abs(x[1]), reverse=True)

# Get top features based on correlation score
top_correlated_features = [feature[0] for feature in sorted_features[:k_best_features]]

# Comparison: Counterattacking disadvantages
# Disadvantage 1: SelectKBest assumes linear relationships
# Disadvantage 2: Pearson Correlation only captures linear relationships

# Mitigation: Consider non-linear relationships using SelectKBest
# Advantages: SelectKBest considers non-linear relationships
# Disadvantages: May discard potentially useful features that are not significant by themselves

# Considering the above, we'll select features from SelectKBest
best_features = selected_features_skbest

print("Best Features Selected:")
print(best_features)



import pandas as pd
import numpy as np
from sklearn.feature_selection import SelectKBest, f_classif
from scipy.stats import pearsonr

# Load the dataset
telecom_df = pd.read_csv('telecom_data.csv')

# Assuming the target variable is named 'churn' (1 for churned, 0 for not churned)
X = telecom_df.drop('churn', axis=1)  # Features
y = telecom_df['churn']  # Target

# Identify categorical and datetime features
categorical_features = X.select_dtypes(include=['object']).columns.tolist()
datetime_features = X.select_dtypes(include=['datetime64']).columns.tolist()

# Convert categorical features to dummy variables
X = pd.get_dummies(X, columns=categorical_features)

# Define the number of features to select
k_best_features = 5

# 1. Statistical Methods: SelectKBest
selector = SelectKBest(score_func=f_classif, k=k_best_features)
selector.fit(X, y)

# Get the indices of the selected features
selected_features_indices = selector.get_support(indices=True)

# Get the names of the selected features
selected_features_skbest = X.columns[selected_features_indices]

# 2. Correlation Methods: Pearson Correlation
correlation_scores = []
for column in selected_features_skbest:
    correlation_score = pearsonr(X[column], y)[0]
    correlation_scores.append((column, correlation_score))

# Sort features based on correlation score
sorted_features = sorted(correlation_scores, key=lambda x: abs(x[1]), reverse=True)

# Get top features based on correlation score
top_correlated_features = [feature[0] for feature in sorted_features[:k_best_features]]

# Combine results: Features selected by both SelectKBest and strong correlation
final_features = list(set(selected_features_skbest) & set(top_correlated_features))

print("Final Features Selected:")
print(final_features)



ip_measured_df = measured_df[measured_df["PRODUCT_CATEGORY"] == "IP"]

ip_measured_df.rename(columns={"SRVC_ORDER_UNIT": "PRIMARY_ORDER_ITEM"}, inplace=True)
ip_measured_df.columns = ip_measured_df.columns.str.upper()
df_1.columns = df_1.columns.str.upper()
df_6.columns = df_6.columns.str.upper()
df_raw = df_1.merge(ip_measured_df, on="PRIMARY_ORDER_ITEM", how="left")
df_5.rename(columns={"DERIVED_SOU": "PRIMARY_ORDER_ITEM"}, inplace=True)
df_7 = df_6.merge(df_5, on=["PRIMARY_ORDER_ITEM", "TASK_NAME"], how="left")
df_7 = df_7.drop_duplicates(subset=["PRIMARY_ORDER_ITEM", "TASK_NAME"], keep="last")
df_7["TASK_STATUS"] = df_7["TASK_STATUS_y"].combine_first(df_7["TASK_STATUS_x"])
df_7.drop(["TASK_STATUS_x", "TASK_STATUS_y"], axis=1, inplace=True)
merge_df = pd.merge(
    df_raw,
    df_7,
    on=[
        "PRIMARY_ORDER_ITEM",
        "TASK_NAME",
        "TASK_STATUS",
        "SERV_ORDER_NBR",
        "ORDER_TYPE",
    ],
    how="left",
)


import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Load and prepare data (replace 'data.csv' with your dataset)
data = pd.read_csv('data.csv')
X = data.drop('target_variable', axis=1)  # Input features
y = data['target_variable']  # Target variable

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Analyze feature importances
feature_importances = pd.DataFrame({'Feature': X.columns, 'Importance': rf_model.feature_importances_})
feature_importances = feature_importances.sort_values(by='Importance', ascending=False)
print("Feature Importances:")
print(feature_importances)

# Select top features for merging
top_features = feature_importances['Feature'].head(5).tolist()  # Select top 5 features
print("Top Features for Merging:")
print(top_features)

# Merge selected columns with other relevant data
# Perform merging based on your specific requirements



import pandas as pd
from sklearn.feature_selection import SelectKBest, f_classif
from scipy.stats import pearsonr

# Load the dataset
telecom_df = pd.read_csv('telecom_data.csv')

# Assuming the target variable is named 'churn' (1 for churned, 0 for not churned)
X = telecom_df.drop('churn', axis=1)  # Features
y = telecom_df['churn']  # Target

# Identify categorical and datetime features
categorical_features = X.select_dtypes(include=['object']).columns.tolist()
datetime_features = X.select_dtypes(include=['datetime64']).columns.tolist()

# Drop categorical and datetime features for feature selection
X_for_selection = X.drop(categorical_features + datetime_features, axis=1)

# Define the number of features to select
k_best_features = 5

# Define the number of features to select in each batch
batch_size = 10

# Initialize an empty list to store selected features
selected_features = []

# Perform incremental feature selection in batches
for i in range(0, X_for_selection.shape[1], batch_size):
    # Select features in the current batch using SelectKBest
    selector = SelectKBest(score_func=f_classif, k=batch_size)
    selector.fit(X_for_selection.iloc[:, i:i+batch_size], y)
    
    # Get the indices of the selected features in the current batch
    selected_indices = selector.get_support(indices=True)
    
    # Get the names of the selected features and add them to the list
    selected_features.extend(X_for_selection.columns[i:i+batch_size][selected_indices])

# 2. Correlation Methods: Pearson Correlation
correlation_scores = []
for column in selected_features:
    correlation_score = pearsonr(X[column], y)[0]
    correlation_scores.append((column, correlation_score))

# Sort features based on correlation score
sorted_features = sorted(correlation_scores, key=lambda x: abs(x[1]), reverse=True)

# Get top features based on correlation score
top_correlated_features = [feature[0] for feature in sorted_features[:k_best_features]]

# Combine results: Features selected by both SelectKBest and strong correlation
final_features = list(set(selected_features) & set(top_correlated_features))

# Print final features selected
print("Final Features Selected:")
print(final_features)


from sklearn.ensemble import RandomForestClassifier
import pandas as pd
import sqlite3

# Load the dataset
conn = sqlite3.connect('your_database.db')  # Connect to your SQLite database
query = 'SELECT * FROM telecom_data'  # Define your SQL query

# Initialize an empty DataFrame to store feature importances
feature_importance_df = pd.DataFrame(columns=['Feature', 'Importance'])

# Loop through chunks of data from the database
for chunk in pd.read_sql_query(query, conn, chunksize=chunk_size):
    # Handle NaN values in the chunk
    chunk.fillna(0, inplace=True)  # Replace NaN values with 0, you can choose another strategy based on your data
    
    # Separate features and target
    X = chunk.drop('churn', axis=1)  # Features
    y = chunk['churn']  # Target
    
    # Identify categorical and datetime features
    categorical_features = X.select_dtypes(include=['object']).columns.tolist()
    datetime_features = X.select_dtypes(include=['datetime64']).columns.tolist()

    # Convert categorical features to dummy variables
    X = pd.get_dummies(X, columns=categorical_features)

    # Include datetime features for column-wise transformation
    X = pd.get_dummies(X, columns=datetime_features)

    # Train RandomForestClassifier
    rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf_clf.fit(X, y)

    # Extract feature importances
    feature_importances = rf_clf.feature_importances_

    # Create a DataFrame to store feature importances for this chunk
    chunk_feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importances})

    # Concatenate with the overall feature importance DataFrame
    feature_importance_df = pd.concat([feature_importance_df, chunk_feature_importance_df], ignore_index=True)

# Close the database connection
conn.close()

# Aggregate feature importances across all chunks
feature_importance_agg = feature_importance_df.groupby('Feature')['Importance'].mean().reset_index()

# Sort features by importance
feature_importance_agg = feature_importance_agg.sort_values(by='Importance', ascending=False)

# Print the top features
print("Top Features Selected:")
print(feature_importance_agg.head())

# Optionally, select top features based on a threshold or a fixed number
# For example, select top 5 features
top_features = feature_importance_agg['Feature'].head(5).tolist()
print("\nTop 5 Features:")
print(top_features)



import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Example DataFrames
df1 = pd.DataFrame({
    'A': [1, 2, 3],
    'B': [4, 5, 6],
    'C': [7, 8, 9]
})

df2 = pd.DataFrame({
    'X': [10, 11, 12],
    'Y': [13, 14, 15],
    'Z': [16, 17, 18]
})

# Combine DataFrames
combined_df = pd.concat([df1, df2], axis=1)

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(combined_df)

# Apply K-means clustering
kmeans = KMeans(n_clusters=2, random_state=42)
clusters = kmeans.fit_predict(scaled_data)

# Add cluster labels to the DataFrame
combined_df['Cluster'] = clusters

# Separate columns based on clusters
cluster_0_cols = combined_df[combined_df['Cluster'] == 0].drop('Cluster', axis=1)
cluster_1_cols = combined_df[combined_df['Cluster'] == 1].drop('Cluster', axis=1)

# Perform left merge based on cluster
merged_df = pd.merge(df1, cluster_0_cols, left_index=True, right_index=True, how='left', suffixes=('_df1', '_cluster_0'))

print(merged_df)


# Encode datetime features using periodic feature encoding
for column in df.columns:
    if df[column].dtype == 'datetime64[ns]':
        df[column + '_hour_of_day'] = np.sin(2 * np.pi * df[column].dt.hour / 24)
        df[column + '_day_of_week'] = np.sin(2 * np.pi * df[column].dt.dayofweek / 7)
        df[column + '_day_of_month'] = np.sin(2 * np.pi * df[column].dt.day / 30)  # Assuming a month has 30 days

# Optionally, drop the original timestamp columns if not needed
df = df.drop(columns=[col for col in data.keys()])


# Check for missing values in BLK_CREATED_DTTM and BLK_MODIFIED_DTTM columns
missing_created = df['BLK_CREATED_DTTM'].isnull()
missing_modified = df['BLK_MODIFIED_DTTM'].isnull()

# Handle missing values
if missing_created.any() or missing_modified.any():
    # Fill missing values with zero
    df['Block_Duration'] = np.nan  # or any other default value
else:
    # Calculate Block_Duration when no missing values
    df['Block_Duration'] = (df['BLK_MODIFIED_DTTM'] - df['BLK_CREATED_DTTM']).dt.total_seconds() / 3600

# Drop rows with missing values if needed
# df.dropna(subset=['Block_Duration'], inplace=True)

import numpy as np

# Check for missing values in BLK_CREATED_DTTM column
missing_created = df['BLK_CREATED_DTTM'].isnull()

# Handle missing values
if missing_created.any():
    # Fill missing values with zero or any other default value
    df['Time_Since_Last_Block'] = np.nan  # or any other default value
else:
    # Calculate Time_Since_Last_Block when no missing values
    df['Time_Since_Last_Block'] = df.groupby('Order_ID')['BLK_CREATED_DTTM'].diff().dt.total_seconds() / 3600

# Drop rows with missing values if needed
# df.dropna(subset=['Time_Since_Last_Block'], inplace=True)

avg_block_duration = df.groupby('Order_ID')['Block_Duration'].mean()
df['Average_Block_Duration'] = df['Order_ID'].map(avg_block_duration)


df['Block_Occurred'] = (~df[block_columns].isnull()).any(axis=1).astype(int)

