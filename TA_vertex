df = pd.read_excel(r"/home/AD17526/Denesh/ML_Automation/Tech Assist Call - Jan'23 MTD (1).xlsx",sheet_name="TA - Jan'23 MTD")
data = df[df['L3'] != None]
data = data.dropna(subset=["L3"])
data = data[(data['L3'] == 'POD Activation/Update/Inquiry')|(data['L3'] == '360 Wifi/Plume Reg')|
           (data['L3'] == 'Linked ONT')|(data['L3'] == 'SmartNID Registration')|
           (data['L3'] == 'Verify/Inquiry Facility')|(data['L3'] == 'Company Req')|
           (data['L3'] == 'Registered Casa')|(data['L3'] == 'VCI Issue')|
           (data['L3'] == 'Cust Requested')|(data['L3'] == 'Update Ticket Status')|
           (data['L3'] == 'ONT Authentication')]

target_keys_list = list(data['L3'].unique())
target_keys = {}
for idx in range(len(target_keys_list)):
    val = target_keys_list[idx]
    target_keys[val] = idx


#clean raw data, taking notes as the data:
nlp = spacy.load('en_core_web_lg-3.6.0/en_core_web_lg/en_core_web_lg-3.6.0')
special_characters = ['.','-', ',','?','/',  '(', ')', ':',  '#', '=', '&', ';', '\xa0', '+', '~', '@', ']', '\\', '_', '!', '*', '>', '$', '|', '"', '[', '%', '{', '}']
for idx in special_characters:
    punc = string.punctuation + idx
    
def text_preprocess(text):
    text = text.lower()
    matches = datefinder.find_dates(text,source=True)
    try:
        matched_string = [match_string for decoded_date,match_string in matches]
    except:
        matched_string = []
    if len(matched_string) > 0:
        for date_string_idx in matched_string: 
            text = text.replace(date_string_idx,'').strip()
    else:
        pass
    doc = nlp(text)
    auto_tagged = [{"text":ent.text,"tag":ent.label_} for ent in doc.ents]
    for idx in auto_tagged: 
        if idx['tag'] != 'ORG':
            text = text.replace(idx["text"],'').strip()
        else:
            pass
        
    split_text = text.split()
    new_text = ''
    for word in split_text:
        sub_word = ''
        for char in word:
            if char not in punc:
                sub_word+=char
            else:
                sub_word+=' '
        new_text = new_text + ' '+sub_word
    text = new_text.replace('  ',' ').strip()
    doc = nlp(text)
    text = [ent.lemma_ for ent in doc]
    new_stopwords = [' ','i', 'me', 'my', 'myself', 'we', 'our', 'ours',
                     'ourselves', 'you', "you're", "you've", "you'll", 
                     "you'd", 'your', 'yours', 'yourself', 'yourselves', 
                     'he', 'him', 'his', 'himself', 'she', "she's", 'her', 
                     'hers', 'herself', 'it', "it's", 'its', 'itself',
                     'they', 'them', 'their', 'theirs', 'themselves',
                     'what', 'which', 'who', 'whom', 'this', 'that',
                     "that'll", 'these', 'those', 'am', 'is', 'are', 
                     'was', 'were', 'be', 'been', 'being', 'have', 'has',
                     'had', 'having', 'do', 'does', 'did', 'doing', 'a',
                     'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as',
                     'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',
                     'against', 'between', 'into', 'through', 'during', 'before',
                     'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in',
                     'out', 'on', 'off', 'over', 'under', 'again', 'further', 
                     'then', 'once', 'here', 'there', 'when', 'where', 
                     'why', 'how', 'all', 'any', 'both', 'each', 'few', 
                     'more', 'most', 'other', 'some', 'such', 'no', 'nor', 
                     'only', 'own', 'same', 'so', 'than', 'too', 'very', 's',
                     't', 'can', 'will', 'just', 'don', "don't", 'should', 
                     "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y',
                     'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't",
                     'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't",
                     'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't",
                     'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn',
                     "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't",
                    "tech","tt","ttt"]
    text = [char for char in text if char not in new_stopwords]
    text_ = []
    for char in text:
        if char.isalpha() == True:
            text_.append(char)
        else:
            pass
    text = ' '.join(text_)
    return text



#get glove embeddings:
embeded_dict = {}
with open(r"/home/AD17526/Denesh/ML_Automation/glove.6B.300d.txt","rb") as f:
    for line in f:
        value = line.split()
        word = value[0]
        vector = np.asarray(value[1:],"float32")
        embeded_dict[word] = vector

glove_wiki = embeded_dict
glove_wiki.keys()
new_glove = {}
for key,val in glove_wiki.items():
    new_key = str(key)
    new_glove[new_key] = val
    
def get_glove(text,size,vectors,aggregation='mean'):
    vec = np.zeros(size).reshape((1,size))
    count = 0
    for word in text.split():
        vect_word = f"b'{word}'"
        try:
            vec+=vectors[vect_word].reshape((1,size))
            count+=1
        except KeyError:
            continue
    if aggregation == 'mean':
        if count!=0:
            vec/=count
        return vec

model_path = "tech_assist_classifier.pkl"
with open(model_path,"rb") as f:
    model = pickle.load(f)

try:
    yesterday = str(date.today() - timedelta(days=1))
    query=f"""select CaseNumber,Notes__c,Description,CreatedDate,AccountId,Case_Type__c,Subtype__c,RecordTypeId,Origin,ClosedDate from Case where  Case_Type__c='Tech Assist' and CreatedDate > {yesterday}T00:00:00.000-06:00 and CreatedDate < {yesterday}T23:59:59.999-06:00"""
    # CaseRecordType,CaseType,SubType,Status
    test_df=pd.DataFrame((sf.query_all(query))['records']).drop(columns=['attributes'])
    test_df.head(n=2)
except KeyError:
    yesterday = str(date.today() - timedelta(days=2))
    query=f"""select CaseNumber,Notes__c,Description,CreatedDate,AccountId,Case_Type__c,Subtype__c,RecordTypeId,Origin,ClosedDate from Case where  Case_Type__c='Tech Assist' and CreatedDate > {yesterday}T00:00:00.000-06:00 and CreatedDate < {yesterday}T23:59:59.999-06:00"""
    # CaseRecordType,CaseType,SubType,Status
    test_df=pd.DataFrame((sf.query_all(query))['records']).drop(columns=['attributes'])
    test_df.head(n=2)

test_df['Notes'] = test_df['Notes__c']
test_df = test_df.dropna(subset=['Notes'])
print(test_df.shape)
tqdm.pandas(desc = 'text cleaning in progress')
test_df['clean_text'] = test_df['Notes'].progress_apply(lambda x: text_preprocess(x))
x_t = list(test_df['clean_text'])
x_t = np.concatenate([get_glove(z,300,new_glove,'mean') for z in x_t])
x_t = pd.DataFrame(x_t)
pred_n = model.predict(x_t)
test_df['pred_class'] = pred_n
target_keys
reverse_target = {}
for key,value in target_keys.items():
    reverse_target[value] = key
test_df['prediction'] = test_df['pred_class'].map(reverse_target)


def get_cluster_groups(df, yesterday):
    save_file_as = f"TECH_Assist_issue_categorization_{yesterday}.xlsx"
    df.to_excel(save_file_as,index = False)
    df['cluster_name'] = df['prediction']
    cluster_counts = df['cluster_name'].value_counts().reset_index()
    cluster_counts.columns = ['cluster_name', 'count']
    first_descriptions = df.groupby('cluster_name')['Notes'].first().reset_index()
    result = pd.merge(cluster_counts, first_descriptions, on='cluster_name')
    result = result.sort_values('count', ascending=False)
    print(result)
    workbook = openpyxl.load_workbook(save_file_as)
    sheet = workbook.create_sheet('Top Categories')
    for row in dataframe_to_rows(result):
        sheet.append(row)
    workbook.save(save_file_as)
    for idx in result['cluster_name'][0:10]:
        df_ = df[df['cluster_name'] == idx]
        desc = create_worksheet_description(save_file_as,idx,df_)
    print(f"result successfully saved at {save_file_as}")
    return workbook, save_file_as


@skill
def process_tech_assist_cases(df: pd.DataFrame):
    """
    Processes the Tech Assist cases DataFrame and returns predictions.
    Args:
        df (pd.DataFrame): DataFrame containing Tech Assist cases.

    Returns:
        pd.DataFrame: DataFrame with predictions.
    """
    import os
    import pandas as pd
    import numpy as np
    import pickle
    import spacy
    import string
    import datefinder
    from datetime import date, timedelta
    from tqdm import tqdm

    # Load spaCy model
    nlp = spacy.load('en_core_web_lg')

    # Load GloVe embeddings
    embeded_dict = {}
    with open(r"/home/AD17526/Denesh/ML_Automation/glove.6B.300d.txt", "rb") as f:
        for line in f:
            value = line.split()
            word = value[0].decode("utf-8")
            vector = np.asarray(value[1:], "float32")
            embeded_dict[word] = vector

    new_glove = {str(key): val for key, val in embeded_dict.items()}

    # Load model
    model_path = "tech_assist_classifier.pkl"
    with open(model_path, "rb") as f:
        model = pickle.load(f)

    # Function to preprocess text
    def text_preprocess(text):
        # Your text preprocessing logic here
        pass

    # Function to get GloVe embeddings
    def get_glove(text, size, vectors, aggregation='mean'):
        # Your GloVe embedding logic here
        pass

    # Your Tech Assist case processing logic here
    def process_cases(df):
        # Your case processing logic here
        pass

    return process_cases(df)

os.environ["LD_LIBRARY_PATH"] = os.pathsep.join([oracle_instant_client_path, os.environ.get("LD_LIBRARY_PATH", "")])

sudo sh -c "echo /opt/oracle/instantclient_21_1 > /etc/ld.so.conf.d/oracle-instantclient.conf"
sudo ldconfig

DatabaseError: (cx_Oracle.DatabaseError) ORA-12170: TNS:Connect timeout occurred
(Background on this error at: https://sqlalche.me/e/20/4xp6)



df = pd.read_excel("transcripts_0416 (1) 1.xlsx")
df = df[0:1000]
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser,JsonOutputParser
from langchain_google_vertexai import VertexAI,VertexAIEmbeddings
llm = VertexAI(model = "gemini-1.5-pro-preview-0409",top_k=5,top_p=0.9,temperature=0.2,max_output_tokens=2048)
template = """The following is a telephonic conversation between a telecommunication agent and a customer.
Summarize the given conversation in one paragraph which contains the context of the conversation and identifies 
the issue that the customer has spoken about.
Also identify the context and the issue for which the customer has called the agent,the issue should 
strictly be of one or two words only..
{question}

The response should strictly be in the following format:
Summary:
Context:
Issue Type:
"""
prompt = PromptTemplate.from_template(template)
summary_chain = prompt | llm | StrOutputParser()
from tqdm import tqdm
response_list = []
error_convos = []
for convo in tqdm(df['Conversation']):
    try:
        response = summary_chain.invoke({"question":convo})
        response_list.append(response)
    except:
        response_list.append(None)
        error_convos.append(convo)
df['llm_summary'] = response_list
df.dropna(subset="llm_summary",inplace=True)
summary = []
context = []
issue_type = []
for i in df['llm_summary']:
    try:
        splits = i.replace("\n\n","\cd").split("\cd")
        cd = splits[2]
    except:
        splits = i.split("\n")
    summary.append(splits[0])
    context.append(splits[1])
    issue_type.append(splits[2])

df['summary'] = summary
df['context'] = context
df['issue_type'] = issue_type

@skill
def summarize_conversations(df, llm):
    """
    Summarizes telephonic conversations in a DataFrame and identifies the context and issue type.
    Args:
        df (pd.DataFrame): DataFrame containing telephonic conversations.
        llm: Language model for summarization.
    Returns:
        pd.DataFrame: DataFrame with added columns for summary, context, and issue type.
    """
    template = """The following is a telephonic conversation between a telecommunication agent and a customer.
    Summarize the given conversation in one paragraph which contains the context of the conversation and identifies 
    the issue that the customer has spoken about.
    Also identify the context and the issue for which the customer has called the agent, the issue should 
    strictly be of one or two words only..
    {question}

    The response should strictly be in the following format:
    Summary:
    Context:
    Issue Type:
    """
    prompt = PromptTemplate.from_template(template)
    summary_chain = prompt | llm | StrOutputParser()

    response_list = []
    error_convos = []
    for convo in tqdm(df['Conversation']):
        try:
            response = summary_chain.invoke({"question": convo})
            response_list.append(response)
        except:
            response_list.append(None)
            error_convos.append(convo)

    df['llm_summary'] = response_list
    df.dropna(subset=["llm_summary"], inplace=True)

    summary = []
    context = []
    issue_type = []
    for i in df['llm_summary']:
        try:
            splits = i.replace("\n\n", "\cd").split("\cd")
            cd = splits[2]
        except:
            splits = i.split("\n")
        summary.append(splits[0])
        context.append(splits[1])
        issue_type.append(splits[2])

    df['summary'] = summary
    df['context'] = context
    df['issue_type'] = issue_type

    return df


# Example usage with Agent
# Load the DataFrame and language model
df = pd.read_excel("transcripts_0416 (1) 1.xlsx")
df = df[0:1000]
llm = VertexAI(model="gemini-1.5-pro-preview-0409", top_k=5, top_p=0.9, temperature=0.2, max_output_tokens=2048)

# Create agent and add the skill
agent = Agent([df], memory_size=10)
agent.add_skills(summarize_conversations)

# Chat with the agent
response = agent.chat("Can you summarize these telephonic conversations?")

IOException: IO Error: Could not set lock on file "/home/jupyter/techbuddy/csv_chat/cache/cache_db_0.9.db": Conflicting lock is held in /opt/conda/bin/python3.10 (PID 19771)
Agent.add_skills() got an unexpected keyword argument 'llm'
google.api_core.exceptions.ResourceExhausted: 429 Unable to submit request because the service is temporarily out of capacity. Try again later.

import base64
import vertexai
from vertexai.generative_models import GenerativeModel, Part, FinishReason
import vertexai.preview.generative_models as generative_models

def generate():
  vertexai.init(project="prj-adapt-ai-dev-001", location="us-west1")
  model = GenerativeModel("gemini-1.0-pro-002")
  responses = model.generate_content(
      [text1],
      generation_config=generation_config,
      safety_settings=safety_settings,
      stream=True,
  )

  for response in responses:
    print(response.text, end="")

text1 = """Summarize the following conversation from the Agent\'s perspective:
Agent: Thank you for calling Google Cloud Support. How may I assist you today?

Customer: Hi, I am trying to create a Google Cloud account and use the free credits, but I am not sure where to start.

Agent: Sure, I can definitely help you with that. May I know if you already have a Google account?

Customer: Yes, I do.

Agent: Great. The first step would be to sign in to the Google Cloud Console. Do you know how to access it?

Customer: No, I don\'t. Can you guide me?

Agent: Sure. Please go to console.cloud.google.com and sign in with your Google account credentials. Once you are signed in, you will be directed to the Google Cloud Console dashboard.

Customer: Okay, I have signed in. What should I do next?

Agent: Now, you\'ll need to create a project. A project is a collection of resources, such as Compute Engine instances and Cloud Storage buckets. To create a project, click on the \"Projects\" tab and then click on the \"Create project\" button. You\'ll need to provide a name for your project and then select a region. Once you\'ve created your project, you\'ll be able to start using the free credits.

Customer: Great, I\'ve created my project. Now what?

Agent: Now, you\'ll need to enable the free trial. To do this, click on the \"Billing\" tab and then click on the \"Enable free trial\" button. You\'ll need to provide your credit card information and then click on the \"Submit\" button.

Customer: Alright, I think I can do that.

Agent: Excellent. You\'ve now successfully created a Google Cloud account. You can start using the free credits that are available by clicking on the \"Credits\" tab.

Customer: That sounds easy enough. How much free credit do I get?

Agent: You\'ll receive $300 worth of free credit that you can use on any Google Cloud services for up to 12 months.

Customer: Is there anything else I should know?

Agent: Yes, just be sure to keep an eye on your usage during your free trial period. Once your credits are used up, you\'ll start being charged for any additional usage. But don\'t worry, you\'ll receive notifications when you\'re approaching your credit limit.

Customer: Thank you so much for your help!

Agent: You\'re welcome. Is there anything else I can help you with today?

Customer: No, that\'s all. Thank you again!

Agent: You\'re welcome. Have a great day and enjoy using Google Cloud!"""

generation_config = {
    "max_output_tokens": 1024,
    "temperature": 0.2,
    "top_p": 0.8,
}

safety_settings = {
    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
}

generate()




import pandas as pd
from langchain.chains import ConversationChain
from langchain.chat_models import ChatVertexAI
from langchain.memory import ConversationBufferMemory
import tiktoken
from vertexai.generative_models import GenerativeModel, Part, FinishReason
import vertexai.preview.generative_models as generative_models

encoder = tiktoken.get_encoding("cl100k_base")

def summarize_conversations(df):
    def summarize_text(text, recursion_level=0):
        # Split text into chunks of 4096 tokens
        texts = []
        text_tokens = encoder.encode(text)
        for i in range(0, len(text_tokens), 4096):
            texts.append(encoder.decode(text_tokens[i:i+4096]))

        llm = ChatVertexAI(max_output_tokens=1024)
        conversation = ConversationChain(llm=llm, memory=ConversationBufferMemory())

        summarized_text = ""
        for i, text_segment in enumerate(texts):
            response = conversation.predict(input="Write a summary of the following text:\n\n" + text_segment)
            summarized_text += response + "\n\n"
        if len(texts) == 1:
            return summarized_text
        else:
            return summarize_text(summarized_text, recursion_level=recursion_level+1)

    # Read conversations from DataFrame
    conversations = df['conversations'].tolist()

    summarized_conversations = []
    for conversation in conversations:
        summarized_conversations.append(summarize_text(conversation))

    return summarized_conversations

def generate(summarized_texts):
    vertexai.init(project="prj-adapt-ai-dev-001", location="us-west1")
    model = GenerativeModel("gemini-1.0-pro-002")
    for text in summarized_texts:
        responses = model.generate_content(
            [text],
            generation_config={
                "max_output_tokens": 1024,
                "temperature": 0.2,
                "top_p": 0.8,
            },
            safety_settings={
                generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            },
            stream=True,
        )

        for response in responses:
            print(response.text, end="")

# Read DataFrame
df = pd.read_excel("transcripts_0416 (1) 1.xlsx")

# Summarize conversations
summarized_texts = summarize_conversations(df)

# Generate content
generate(summarized_texts)

HTTPSConnectionPool(host='openaipublic.blob.core.windows.net', port=443): Max retries exceeded with url: /encodings/cl100k_base.tiktoken (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f8cf9a6ad40>, 'Connection to openaipublic.blob.core.windows.net timed out. (connect timeout=None)'))

def rag_qa_sys(question):     """QA Rag sys"""          rag_prompt = """You are chatting with a conversational AI named TechBuddy that can answer questions based on the context provided. \     The AI is designed to provide relevant responses to user question.\     Question:     {question}     Context:     {context}"""     rag_qa_prompt = PromptTemplate.from_template(rag_prompt)     rag_chain = ({"context":retriever | format_docs , "question": RunnablePassthrough()}                  | rag_qa_prompt                  | llm                  | StrOutputParser())     rag_chain_from_docs = (         RunnablePassthrough.assign(context=(lambda x: format_docs(x["context"])))         | rag_qa_prompt         | llm         | StrOutputParser()     )     rag_chain_from_source = RunnableParallel({"context":retriever,"question":RunnablePassthrough()}).assign(answer=rag_chain_from_docs)     resp = rag_chain_from_source.invoke({"question":question})     return resp Take the input df(issue_type) for question and return the answer in the same df by creating a new column sol_fix, if the answer is not available return none
#rag definition:

@tool

def rag_qa_sys(question):

    """QA Rag sys"""

    rag_prompt = """You are chatting with a conversational AI named TechBuddy that can answer questions based on the context provided. \

    The AI is designed to provide relevant responses to user question.\

    Question:

    {question}
 
    Context:

    {context}"""
 
ake the input df(issue_type) for question and return the answer in the same df by creating a new column sol_fix, if the answer is not available return none
 
    rag_qa_prompt = PromptTemplate.from_template(rag_prompt)
 
    rag_chain = ({"context":retriever | format_docs , "question": RunnablePassthrough()} 

                 | rag_qa_prompt 

                 | llm 

                 | StrOutputParser())
 
    rag_chain_from_docs = (

        RunnablePassthrough.assign(context=(lambda x: format_docs(x["context"])))

        | rag_qa_prompt

        | llm

        | StrOutputParser()

    )

    rag_chain_from_source = RunnableParallel({"context":retriever,"question":RunnablePassthrough()}).assign(answer=rag_chain_from_docs)

    resp = rag_chain_from_source.invoke({"question":question})

    return resp

import pandas as pd
from langchain.chains import ConversationChain
from langchain.chat_models import ChatVertexAI
from langchain.memory import ConversationBufferMemory

class BPETokenizer:
    def __init__(self, vocab_size=1000):
        self.vocab_size = vocab_size
        self.vocab = {}
    
    def train(self, corpus):
        # Count pairs of characters
        pairs = {}
        for word in corpus:
            for i in range(len(word) - 1):
                pair = word[i:i+2]
                if pair in pairs:
                    pairs[pair] += 1
                else:
                    pairs[pair] = 1
        
        # Merge most frequent pairs until vocabulary size is reached
        for _ in range(self.vocab_size):
            max_pair = max(pairs, key=pairs.get)
            self._merge_pair(max_pair)
            del pairs[max_pair]
    
    def _merge_pair(self, pair):
        # Merge the most frequent pair into the vocabulary
        new_token = ''.join(pair)
        self.vocab[new_token] = len(self.vocab)
        
        # Replace the pair with the new token in the corpus
        for i, word in enumerate(corpus):
            corpus[i] = word.replace(pair, new_token)
    
    def tokenize(self, text):
        tokens = []
        i = 0
        while i < len(text):
            found_token = False
            for token in self.vocab:
                if text[i:i+len(token)] == token:
                    tokens.append(token)
                    i += len(token)
                    found_token = True
                    break
            if not found_token:
                tokens.append(text[i])
                i += 1
        return tokens

def summarize_conversations(df):
    def summarize_text(text, recursion_level=0):
        # Split text into chunks of 4096 tokens
        texts = []
        text_tokens = tokenizer.tokenize(text)
        for i in range(0, len(text_tokens), 4096):
            texts.append(''.join(text_tokens[i:i+4096]))

        llm = ChatVertexAI(max_output_tokens=1024)
        conversation = ConversationChain(llm=llm, memory=ConversationBufferMemory())

        summarized_text = ""
        for i, text_segment in enumerate(texts):
            response = conversation.predict(input="Write a summary of the following text:\n\n" + text_segment)
            summarized_text += response + "\n\n"
        if len(texts) == 1:
            return summarized_text
        else:
            return summarize_text(summarized_text, recursion_level=recursion_level+1)

    # Read conversations from DataFrame
    conversations = df['conversations'].tolist()

    summarized_conversations = []
    for conversation in conversations:
        summarized_conversations.append(summarize_text(conversation))

    return summarized_conversations

def generate(summarized_texts):
    model = GenerativeModel("gemini-1.0-pro-002")
    for text in summarized_texts:
        responses = model.generate_content(
            [text],
            generation_config={
                "max_output_tokens": 1024,
                "temperature": 0.2,
                "top_p": 0.8,
            },
            safety_settings={
                generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            },
            stream=True,
        )

        for response in responses:
            print(response.text, end="")

# Load tokenizer
tokenizer = BPETokenizer(vocab_size=1000)
corpus = ["low", "lower", "newest", "widest"]
tokenizer.train(corpus)

# Read DataFrame
df = pd.read_excel("transcripts_0416 (1) 1.xlsx")

# Summarize conversations
summarized_texts = summarize_conversations(df)

# Generate content
generate(summarized_texts)

Use case: To fetch customer details from database and generate responsive answers through natural language which will be used for Gen AI chatbot.

Reduction in Task Resolution Time and Task Resolution Rate, Reduction in Number of Delayed Tasks and Average Delay Time 


from langchain_core.runnables import RunnableParallel,RunnablePassthrough
 
 
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)
 
rag_prompt_template = """Answer questions based on the context provided. The AI is designed to provide relevant responses to user question.
Question:
{question}
 
Context:
{context}"""
 
rag_prompt = PromptTemplate.from_template(rag_prompt_template)
rag_chain_from_docs = (
RunnablePassthrough.assign(context=(lambda x: format_docs(x['context'])))
| rag_prompt
| llm
| StrOutputParser()
)
 
rag_chain_with_source = RunnableParallel(
{"context":retriever, "question":RunnablePassthrough()}).assign(answer=rag_chain_from_docs)


import os
import pandas as pd

# Function to get all file paths with a specific name in a directory and its subdirectories
def get_file_paths(root_dir, file_name):
    file_paths = []
    for root, dirs, files in os.walk(root_dir):
        for file in files:
            if file == file_name:
                file_paths.append(os.path.join(root, file))
    return file_paths

# Paths to the three month folders
month_folders = ['september', 'october', 'november']
consolidated_reports = []

# Iterate over each month folder
for month in month_folders:
    month_path = os.path.join('path_to_parent_directory', month)
    
    # Get all paths for consolidated_daily_dispatch_V1 reports in the month folder
    report_paths = get_file_paths(month_path, 'consolidated_daily_dispatch_V1.xlsx')
    
    # Read each report and append to the consolidated_reports list
    for report_path in report_paths:
        report_df = pd.read_excel(report_path)
        consolidated_reports.append(report_df)

# Concatenate all reports into a single DataFrame
final_report = pd.concat(consolidated_reports, ignore_index=True)

# Write the concatenated DataFrame to a new Excel file
final_report.to_excel('path_to_save/final_consolidated_report.xlsx', index=False)

print("Final consolidated report saved successfully.")

