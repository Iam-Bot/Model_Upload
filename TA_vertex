df = pd.read_excel(r"/home/AD17526/Denesh/ML_Automation/Tech Assist Call - Jan'23 MTD (1).xlsx",sheet_name="TA - Jan'23 MTD")
data = df[df['L3'] != None]
data = data.dropna(subset=["L3"])
data = data[(data['L3'] == 'POD Activation/Update/Inquiry')|(data['L3'] == '360 Wifi/Plume Reg')|
           (data['L3'] == 'Linked ONT')|(data['L3'] == 'SmartNID Registration')|
           (data['L3'] == 'Verify/Inquiry Facility')|(data['L3'] == 'Company Req')|
           (data['L3'] == 'Registered Casa')|(data['L3'] == 'VCI Issue')|
           (data['L3'] == 'Cust Requested')|(data['L3'] == 'Update Ticket Status')|
           (data['L3'] == 'ONT Authentication')]

target_keys_list = list(data['L3'].unique())
target_keys = {}
for idx in range(len(target_keys_list)):
    val = target_keys_list[idx]
    target_keys[val] = idx


#clean raw data, taking notes as the data:
nlp = spacy.load('en_core_web_lg-3.6.0/en_core_web_lg/en_core_web_lg-3.6.0')
special_characters = ['.','-', ',','?','/',  '(', ')', ':',  '#', '=', '&', ';', '\xa0', '+', '~', '@', ']', '\\', '_', '!', '*', '>', '$', '|', '"', '[', '%', '{', '}']
for idx in special_characters:
    punc = string.punctuation + idx
    
def text_preprocess(text):
    text = text.lower()
    matches = datefinder.find_dates(text,source=True)
    try:
        matched_string = [match_string for decoded_date,match_string in matches]
    except:
        matched_string = []
    if len(matched_string) > 0:
        for date_string_idx in matched_string: 
            text = text.replace(date_string_idx,'').strip()
    else:
        pass
    doc = nlp(text)
    auto_tagged = [{"text":ent.text,"tag":ent.label_} for ent in doc.ents]
    for idx in auto_tagged: 
        if idx['tag'] != 'ORG':
            text = text.replace(idx["text"],'').strip()
        else:
            pass
        
    split_text = text.split()
    new_text = ''
    for word in split_text:
        sub_word = ''
        for char in word:
            if char not in punc:
                sub_word+=char
            else:
                sub_word+=' '
        new_text = new_text + ' '+sub_word
    text = new_text.replace('  ',' ').strip()
    doc = nlp(text)
    text = [ent.lemma_ for ent in doc]
    new_stopwords = [' ','i', 'me', 'my', 'myself', 'we', 'our', 'ours',
                     'ourselves', 'you', "you're", "you've", "you'll", 
                     "you'd", 'your', 'yours', 'yourself', 'yourselves', 
                     'he', 'him', 'his', 'himself', 'she', "she's", 'her', 
                     'hers', 'herself', 'it', "it's", 'its', 'itself',
                     'they', 'them', 'their', 'theirs', 'themselves',
                     'what', 'which', 'who', 'whom', 'this', 'that',
                     "that'll", 'these', 'those', 'am', 'is', 'are', 
                     'was', 'were', 'be', 'been', 'being', 'have', 'has',
                     'had', 'having', 'do', 'does', 'did', 'doing', 'a',
                     'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as',
                     'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',
                     'against', 'between', 'into', 'through', 'during', 'before',
                     'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in',
                     'out', 'on', 'off', 'over', 'under', 'again', 'further', 
                     'then', 'once', 'here', 'there', 'when', 'where', 
                     'why', 'how', 'all', 'any', 'both', 'each', 'few', 
                     'more', 'most', 'other', 'some', 'such', 'no', 'nor', 
                     'only', 'own', 'same', 'so', 'than', 'too', 'very', 's',
                     't', 'can', 'will', 'just', 'don', "don't", 'should', 
                     "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y',
                     'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't",
                     'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't",
                     'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't",
                     'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn',
                     "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't",
                    "tech","tt","ttt"]
    text = [char for char in text if char not in new_stopwords]
    text_ = []
    for char in text:
        if char.isalpha() == True:
            text_.append(char)
        else:
            pass
    text = ' '.join(text_)
    return text



#get glove embeddings:
embeded_dict = {}
with open(r"/home/AD17526/Denesh/ML_Automation/glove.6B.300d.txt","rb") as f:
    for line in f:
        value = line.split()
        word = value[0]
        vector = np.asarray(value[1:],"float32")
        embeded_dict[word] = vector

glove_wiki = embeded_dict
glove_wiki.keys()
new_glove = {}
for key,val in glove_wiki.items():
    new_key = str(key)
    new_glove[new_key] = val
    
def get_glove(text,size,vectors,aggregation='mean'):
    vec = np.zeros(size).reshape((1,size))
    count = 0
    for word in text.split():
        vect_word = f"b'{word}'"
        try:
            vec+=vectors[vect_word].reshape((1,size))
            count+=1
        except KeyError:
            continue
    if aggregation == 'mean':
        if count!=0:
            vec/=count
        return vec

model_path = "tech_assist_classifier.pkl"
with open(model_path,"rb") as f:
    model = pickle.load(f)

try:
    yesterday = str(date.today() - timedelta(days=1))
    query=f"""select CaseNumber,Notes__c,Description,CreatedDate,AccountId,Case_Type__c,Subtype__c,RecordTypeId,Origin,ClosedDate from Case where  Case_Type__c='Tech Assist' and CreatedDate > {yesterday}T00:00:00.000-06:00 and CreatedDate < {yesterday}T23:59:59.999-06:00"""
    # CaseRecordType,CaseType,SubType,Status
    test_df=pd.DataFrame((sf.query_all(query))['records']).drop(columns=['attributes'])
    test_df.head(n=2)
except KeyError:
    yesterday = str(date.today() - timedelta(days=2))
    query=f"""select CaseNumber,Notes__c,Description,CreatedDate,AccountId,Case_Type__c,Subtype__c,RecordTypeId,Origin,ClosedDate from Case where  Case_Type__c='Tech Assist' and CreatedDate > {yesterday}T00:00:00.000-06:00 and CreatedDate < {yesterday}T23:59:59.999-06:00"""
    # CaseRecordType,CaseType,SubType,Status
    test_df=pd.DataFrame((sf.query_all(query))['records']).drop(columns=['attributes'])
    test_df.head(n=2)

test_df['Notes'] = test_df['Notes__c']
test_df = test_df.dropna(subset=['Notes'])
print(test_df.shape)
tqdm.pandas(desc = 'text cleaning in progress')
test_df['clean_text'] = test_df['Notes'].progress_apply(lambda x: text_preprocess(x))
x_t = list(test_df['clean_text'])
x_t = np.concatenate([get_glove(z,300,new_glove,'mean') for z in x_t])
x_t = pd.DataFrame(x_t)
pred_n = model.predict(x_t)
test_df['pred_class'] = pred_n
target_keys
reverse_target = {}
for key,value in target_keys.items():
    reverse_target[value] = key
test_df['prediction'] = test_df['pred_class'].map(reverse_target)


def get_cluster_groups(df, yesterday):
    save_file_as = f"TECH_Assist_issue_categorization_{yesterday}.xlsx"
    df.to_excel(save_file_as,index = False)
    df['cluster_name'] = df['prediction']
    cluster_counts = df['cluster_name'].value_counts().reset_index()
    cluster_counts.columns = ['cluster_name', 'count']
    first_descriptions = df.groupby('cluster_name')['Notes'].first().reset_index()
    result = pd.merge(cluster_counts, first_descriptions, on='cluster_name')
    result = result.sort_values('count', ascending=False)
    print(result)
    workbook = openpyxl.load_workbook(save_file_as)
    sheet = workbook.create_sheet('Top Categories')
    for row in dataframe_to_rows(result):
        sheet.append(row)
    workbook.save(save_file_as)
    for idx in result['cluster_name'][0:10]:
        df_ = df[df['cluster_name'] == idx]
        desc = create_worksheet_description(save_file_as,idx,df_)
    print(f"result successfully saved at {save_file_as}")
    return workbook, save_file_as


@skill
def process_tech_assist_cases(df: pd.DataFrame):
    """
    Processes the Tech Assist cases DataFrame and returns predictions.
    Args:
        df (pd.DataFrame): DataFrame containing Tech Assist cases.

    Returns:
        pd.DataFrame: DataFrame with predictions.
    """
    import os
    import pandas as pd
    import numpy as np
    import pickle
    import spacy
    import string
    import datefinder
    from datetime import date, timedelta
    from tqdm import tqdm

    # Load spaCy model
    nlp = spacy.load('en_core_web_lg')

    # Load GloVe embeddings
    embeded_dict = {}
    with open(r"/home/AD17526/Denesh/ML_Automation/glove.6B.300d.txt", "rb") as f:
        for line in f:
            value = line.split()
            word = value[0].decode("utf-8")
            vector = np.asarray(value[1:], "float32")
            embeded_dict[word] = vector

    new_glove = {str(key): val for key, val in embeded_dict.items()}

    # Load model
    model_path = "tech_assist_classifier.pkl"
    with open(model_path, "rb") as f:
        model = pickle.load(f)

    # Function to preprocess text
    def text_preprocess(text):
        # Your text preprocessing logic here
        pass

    # Function to get GloVe embeddings
    def get_glove(text, size, vectors, aggregation='mean'):
        # Your GloVe embedding logic here
        pass

    # Your Tech Assist case processing logic here
    def process_cases(df):
        # Your case processing logic here
        pass

    return process_cases(df)

os.environ["LD_LIBRARY_PATH"] = os.pathsep.join([oracle_instant_client_path, os.environ.get("LD_LIBRARY_PATH", "")])

sudo sh -c "echo /opt/oracle/instantclient_21_1 > /etc/ld.so.conf.d/oracle-instantclient.conf"
sudo ldconfig

DatabaseError: (cx_Oracle.DatabaseError) ORA-12170: TNS:Connect timeout occurred
(Background on this error at: https://sqlalche.me/e/20/4xp6)



df = pd.read_excel("transcripts_0416 (1) 1.xlsx")
df = df[0:1000]
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser,JsonOutputParser
from langchain_google_vertexai import VertexAI,VertexAIEmbeddings
llm = VertexAI(model = "gemini-1.5-pro-preview-0409",top_k=5,top_p=0.9,temperature=0.2,max_output_tokens=2048)
template = """The following is a telephonic conversation between a telecommunication agent and a customer.
Summarize the given conversation in one paragraph which contains the context of the conversation and identifies 
the issue that the customer has spoken about.
Also identify the context and the issue for which the customer has called the agent,the issue should 
strictly be of one or two words only..
{question}

The response should strictly be in the following format:
Summary:
Context:
Issue Type:
"""
prompt = PromptTemplate.from_template(template)
summary_chain = prompt | llm | StrOutputParser()
from tqdm import tqdm
response_list = []
error_convos = []
for convo in tqdm(df['Conversation']):
    try:
        response = summary_chain.invoke({"question":convo})
        response_list.append(response)
    except:
        response_list.append(None)
        error_convos.append(convo)
df['llm_summary'] = response_list
df.dropna(subset="llm_summary",inplace=True)
summary = []
context = []
issue_type = []
for i in df['llm_summary']:
    try:
        splits = i.replace("\n\n","\cd").split("\cd")
        cd = splits[2]
    except:
        splits = i.split("\n")
    summary.append(splits[0])
    context.append(splits[1])
    issue_type.append(splits[2])

df['summary'] = summary
df['context'] = context
df['issue_type'] = issue_type

@skill
def summarize_conversations(df, llm):
    """
    Summarizes telephonic conversations in a DataFrame and identifies the context and issue type.
    Args:
        df (pd.DataFrame): DataFrame containing telephonic conversations.
        llm: Language model for summarization.
    Returns:
        pd.DataFrame: DataFrame with added columns for summary, context, and issue type.
    """
    template = """The following is a telephonic conversation between a telecommunication agent and a customer.
    Summarize the given conversation in one paragraph which contains the context of the conversation and identifies 
    the issue that the customer has spoken about.
    Also identify the context and the issue for which the customer has called the agent, the issue should 
    strictly be of one or two words only..
    {question}

    The response should strictly be in the following format:
    Summary:
    Context:
    Issue Type:
    """
    prompt = PromptTemplate.from_template(template)
    summary_chain = prompt | llm | StrOutputParser()

    response_list = []
    error_convos = []
    for convo in tqdm(df['Conversation']):
        try:
            response = summary_chain.invoke({"question": convo})
            response_list.append(response)
        except:
            response_list.append(None)
            error_convos.append(convo)

    df['llm_summary'] = response_list
    df.dropna(subset=["llm_summary"], inplace=True)

    summary = []
    context = []
    issue_type = []
    for i in df['llm_summary']:
        try:
            splits = i.replace("\n\n", "\cd").split("\cd")
            cd = splits[2]
        except:
            splits = i.split("\n")
        summary.append(splits[0])
        context.append(splits[1])
        issue_type.append(splits[2])

    df['summary'] = summary
    df['context'] = context
    df['issue_type'] = issue_type

    return df


# Example usage with Agent
# Load the DataFrame and language model
df = pd.read_excel("transcripts_0416 (1) 1.xlsx")
df = df[0:1000]
llm = VertexAI(model="gemini-1.5-pro-preview-0409", top_k=5, top_p=0.9, temperature=0.2, max_output_tokens=2048)

# Create agent and add the skill
agent = Agent([df], memory_size=10)
agent.add_skills(summarize_conversations)

# Chat with the agent
response = agent.chat("Can you summarize these telephonic conversations?")

IOException: IO Error: Could not set lock on file "/home/jupyter/techbuddy/csv_chat/cache/cache_db_0.9.db": Conflicting lock is held in /opt/conda/bin/python3.10 (PID 19771)
Agent.add_skills() got an unexpected keyword argument 'llm'
google.api_core.exceptions.ResourceExhausted: 429 Unable to submit request because the service is temporarily out of capacity. Try again later.

import base64
import vertexai
from vertexai.generative_models import GenerativeModel, Part, FinishReason
import vertexai.preview.generative_models as generative_models

def generate():
  vertexai.init(project="prj-adapt-ai-dev-001", location="us-west1")
  model = GenerativeModel("gemini-1.0-pro-002")
  responses = model.generate_content(
      [text1],
      generation_config=generation_config,
      safety_settings=safety_settings,
      stream=True,
  )

  for response in responses:
    print(response.text, end="")

text1 = """Summarize the following conversation from the Agent\'s perspective:
Agent: Thank you for calling Google Cloud Support. How may I assist you today?

Customer: Hi, I am trying to create a Google Cloud account and use the free credits, but I am not sure where to start.

Agent: Sure, I can definitely help you with that. May I know if you already have a Google account?

Customer: Yes, I do.

Agent: Great. The first step would be to sign in to the Google Cloud Console. Do you know how to access it?

Customer: No, I don\'t. Can you guide me?

Agent: Sure. Please go to console.cloud.google.com and sign in with your Google account credentials. Once you are signed in, you will be directed to the Google Cloud Console dashboard.

Customer: Okay, I have signed in. What should I do next?

Agent: Now, you\'ll need to create a project. A project is a collection of resources, such as Compute Engine instances and Cloud Storage buckets. To create a project, click on the \"Projects\" tab and then click on the \"Create project\" button. You\'ll need to provide a name for your project and then select a region. Once you\'ve created your project, you\'ll be able to start using the free credits.

Customer: Great, I\'ve created my project. Now what?

Agent: Now, you\'ll need to enable the free trial. To do this, click on the \"Billing\" tab and then click on the \"Enable free trial\" button. You\'ll need to provide your credit card information and then click on the \"Submit\" button.

Customer: Alright, I think I can do that.

Agent: Excellent. You\'ve now successfully created a Google Cloud account. You can start using the free credits that are available by clicking on the \"Credits\" tab.

Customer: That sounds easy enough. How much free credit do I get?

Agent: You\'ll receive $300 worth of free credit that you can use on any Google Cloud services for up to 12 months.

Customer: Is there anything else I should know?

Agent: Yes, just be sure to keep an eye on your usage during your free trial period. Once your credits are used up, you\'ll start being charged for any additional usage. But don\'t worry, you\'ll receive notifications when you\'re approaching your credit limit.

Customer: Thank you so much for your help!

Agent: You\'re welcome. Is there anything else I can help you with today?

Customer: No, that\'s all. Thank you again!

Agent: You\'re welcome. Have a great day and enjoy using Google Cloud!"""

generation_config = {
    "max_output_tokens": 1024,
    "temperature": 0.2,
    "top_p": 0.8,
}

safety_settings = {
    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
}

generate()




import pandas as pd
from langchain.chains import ConversationChain
from langchain.chat_models import ChatVertexAI
from langchain.memory import ConversationBufferMemory
import tiktoken
from vertexai.generative_models import GenerativeModel, Part, FinishReason
import vertexai.preview.generative_models as generative_models

encoder = tiktoken.get_encoding("cl100k_base")

def summarize_conversations(df):
    def summarize_text(text, recursion_level=0):
        # Split text into chunks of 4096 tokens
        texts = []
        text_tokens = encoder.encode(text)
        for i in range(0, len(text_tokens), 4096):
            texts.append(encoder.decode(text_tokens[i:i+4096]))

        llm = ChatVertexAI(max_output_tokens=1024)
        conversation = ConversationChain(llm=llm, memory=ConversationBufferMemory())

        summarized_text = ""
        for i, text_segment in enumerate(texts):
            response = conversation.predict(input="Write a summary of the following text:\n\n" + text_segment)
            summarized_text += response + "\n\n"
        if len(texts) == 1:
            return summarized_text
        else:
            return summarize_text(summarized_text, recursion_level=recursion_level+1)

    # Read conversations from DataFrame
    conversations = df['conversations'].tolist()

    summarized_conversations = []
    for conversation in conversations:
        summarized_conversations.append(summarize_text(conversation))

    return summarized_conversations

def generate(summarized_texts):
    vertexai.init(project="prj-adapt-ai-dev-001", location="us-west1")
    model = GenerativeModel("gemini-1.0-pro-002")
    for text in summarized_texts:
        responses = model.generate_content(
            [text],
            generation_config={
                "max_output_tokens": 1024,
                "temperature": 0.2,
                "top_p": 0.8,
            },
            safety_settings={
                generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            },
            stream=True,
        )

        for response in responses:
            print(response.text, end="")

# Read DataFrame
df = pd.read_excel("transcripts_0416 (1) 1.xlsx")

# Summarize conversations
summarized_texts = summarize_conversations(df)

# Generate content
generate(summarized_texts)

HTTPSConnectionPool(host='openaipublic.blob.core.windows.net', port=443): Max retries exceeded with url: /encodings/cl100k_base.tiktoken (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f8cf9a6ad40>, 'Connection to openaipublic.blob.core.windows.net timed out. (connect timeout=None)'))

def rag_qa_sys(question):     """QA Rag sys"""          rag_prompt = """You are chatting with a conversational AI named TechBuddy that can answer questions based on the context provided. \     The AI is designed to provide relevant responses to user question.\     Question:     {question}     Context:     {context}"""     rag_qa_prompt = PromptTemplate.from_template(rag_prompt)     rag_chain = ({"context":retriever | format_docs , "question": RunnablePassthrough()}                  | rag_qa_prompt                  | llm                  | StrOutputParser())     rag_chain_from_docs = (         RunnablePassthrough.assign(context=(lambda x: format_docs(x["context"])))         | rag_qa_prompt         | llm         | StrOutputParser()     )     rag_chain_from_source = RunnableParallel({"context":retriever,"question":RunnablePassthrough()}).assign(answer=rag_chain_from_docs)     resp = rag_chain_from_source.invoke({"question":question})     return resp Take the input df(issue_type) for question and return the answer in the same df by creating a new column sol_fix, if the answer is not available return none
#rag definition:

@tool

def rag_qa_sys(question):

    """QA Rag sys"""

    rag_prompt = """You are chatting with a conversational AI named TechBuddy that can answer questions based on the context provided. \

    The AI is designed to provide relevant responses to user question.\

    Question:

    {question}
 
    Context:

    {context}"""
 
ake the input df(issue_type) for question and return the answer in the same df by creating a new column sol_fix, if the answer is not available return none
 
    rag_qa_prompt = PromptTemplate.from_template(rag_prompt)
 
    rag_chain = ({"context":retriever | format_docs , "question": RunnablePassthrough()} 

                 | rag_qa_prompt 

                 | llm 

                 | StrOutputParser())
 
    rag_chain_from_docs = (

        RunnablePassthrough.assign(context=(lambda x: format_docs(x["context"])))

        | rag_qa_prompt

        | llm

        | StrOutputParser()

    )

    rag_chain_from_source = RunnableParallel({"context":retriever,"question":RunnablePassthrough()}).assign(answer=rag_chain_from_docs)

    resp = rag_chain_from_source.invoke({"question":question})

    return resp

import pandas as pd
from langchain.chains import ConversationChain
from langchain.chat_models import ChatVertexAI
from langchain.memory import ConversationBufferMemory

class BPETokenizer:
    def __init__(self, vocab_size=1000):
        self.vocab_size = vocab_size
        self.vocab = {}
    
    def train(self, corpus):
        # Count pairs of characters
        pairs = {}
        for word in corpus:
            for i in range(len(word) - 1):
                pair = word[i:i+2]
                if pair in pairs:
                    pairs[pair] += 1
                else:
                    pairs[pair] = 1
        
        # Merge most frequent pairs until vocabulary size is reached
        for _ in range(self.vocab_size):
            max_pair = max(pairs, key=pairs.get)
            self._merge_pair(max_pair)
            del pairs[max_pair]
    
    def _merge_pair(self, pair):
        # Merge the most frequent pair into the vocabulary
        new_token = ''.join(pair)
        self.vocab[new_token] = len(self.vocab)
        
        # Replace the pair with the new token in the corpus
        for i, word in enumerate(corpus):
            corpus[i] = word.replace(pair, new_token)
    
    def tokenize(self, text):
        tokens = []
        i = 0
        while i < len(text):
            found_token = False
            for token in self.vocab:
                if text[i:i+len(token)] == token:
                    tokens.append(token)
                    i += len(token)
                    found_token = True
                    break
            if not found_token:
                tokens.append(text[i])
                i += 1
        return tokens

def summarize_conversations(df):
    def summarize_text(text, recursion_level=0):
        # Split text into chunks of 4096 tokens
        texts = []
        text_tokens = tokenizer.tokenize(text)
        for i in range(0, len(text_tokens), 4096):
            texts.append(''.join(text_tokens[i:i+4096]))

        llm = ChatVertexAI(max_output_tokens=1024)
        conversation = ConversationChain(llm=llm, memory=ConversationBufferMemory())

        summarized_text = ""
        for i, text_segment in enumerate(texts):
            response = conversation.predict(input="Write a summary of the following text:\n\n" + text_segment)
            summarized_text += response + "\n\n"
        if len(texts) == 1:
            return summarized_text
        else:
            return summarize_text(summarized_text, recursion_level=recursion_level+1)

    # Read conversations from DataFrame
    conversations = df['conversations'].tolist()

    summarized_conversations = []
    for conversation in conversations:
        summarized_conversations.append(summarize_text(conversation))

    return summarized_conversations

def generate(summarized_texts):
    model = GenerativeModel("gemini-1.0-pro-002")
    for text in summarized_texts:
        responses = model.generate_content(
            [text],
            generation_config={
                "max_output_tokens": 1024,
                "temperature": 0.2,
                "top_p": 0.8,
            },
            safety_settings={
                generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
                generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
            },
            stream=True,
        )

        for response in responses:
            print(response.text, end="")

# Load tokenizer
tokenizer = BPETokenizer(vocab_size=1000)
corpus = ["low", "lower", "newest", "widest"]
tokenizer.train(corpus)

# Read DataFrame
df = pd.read_excel("transcripts_0416 (1) 1.xlsx")

# Summarize conversations
summarized_texts = summarize_conversations(df)

# Generate content
generate(summarized_texts)

Use case: To fetch customer details from database and generate responsive answers through natural language which will be used for Gen AI chatbot.

Reduction in Task Resolution Time and Task Resolution Rate, Reduction in Number of Delayed Tasks and Average Delay Time 


from langchain_core.runnables import RunnableParallel,RunnablePassthrough
 
 
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)
 
rag_prompt_template = """Answer questions based on the context provided. The AI is designed to provide relevant responses to user question.
Question:
{question}
 
Context:
{context}"""
 
rag_prompt = PromptTemplate.from_template(rag_prompt_template)
rag_chain_from_docs = (
RunnablePassthrough.assign(context=(lambda x: format_docs(x['context'])))
| rag_prompt
| llm
| StrOutputParser()
)
 
rag_chain_with_source = RunnableParallel(
{"context":retriever, "question":RunnablePassthrough()}).assign(answer=rag_chain_from_docs)


import os
import pandas as pd

# Function to get all file paths with a specific name in a directory and its subdirectories
def get_file_paths(root_dir, file_name):
    file_paths = []
    for root, dirs, files in os.walk(root_dir):
        for file in files:
            if file == file_name:
                file_paths.append(os.path.join(root, file))
    return file_paths

# Paths to the three month folders
month_folders = ['september', 'october', 'november']
consolidated_reports = []

# Iterate over each month folder
for month in month_folders:
    month_path = os.path.join('path_to_parent_directory', month)
    
    # Get all paths for consolidated_daily_dispatch_V1 reports in the month folder
    report_paths = get_file_paths(month_path, 'consolidated_daily_dispatch_V1.xlsx')
    
    # Read each report and append to the consolidated_reports list
    for report_path in report_paths:
        report_df = pd.read_excel(report_path)
        consolidated_reports.append(report_df)

# Concatenate all reports into a single DataFrame
final_report = pd.concat(consolidated_reports, ignore_index=True)

# Write the concatenated DataFrame to a new Excel file
final_report.to_excel('path_to_save/final_consolidated_report.xlsx', index=False)

print("Final consolidated report saved successfully.")




# Read each report and concatenate all sheets into a single DataFrame
    for report_path in report_paths:
        xls = pd.ExcelFile(report_path)
        for sheet_name in xls.sheet_names:
            clean_sheet = clean_sheet_name(sheet_name)
            if clean_sheet not in consolidated_reports:
                consolidated_reports[clean_sheet] = []
            # Read the sheet data and store it in the corresponding key
            df = pd.read_excel(xls, sheet_name)
            consolidated_reports[clean_sheet].append(df)

# Write the concatenated DataFrames to a new Excel file with multiple sheets
with pd.ExcelWriter('path_to_save/final_consolidated_report.xlsx') as writer:
    for sheet_name, dfs in consolidated_reports.items():
        # Concatenate all DataFrames for the current sheet
        concatenated_df = pd.concat(dfs, ignore_index=True)
        concatenated_df.to_excel(writer, sheet_name=sheet_name, index=False)

print("Final consolidated report saved successfully.")



quantum programming this is jolene how can i help you. hey jolene this is jerry out in cheney washington how are you. good and yourself. good yeah i`m working on a ticket and it`s been a problem for us um it`s the d t n is one zero zero zero two four four six one six. what`s your cu id please. it`s a a seven three nine six zero. thank you bear with me a few moments while i bring up the dtn. okay how`s your thursday going. it`s gone i`m bringing everything up here bear with me a second. okay. i`m seeing they moved you from port three to port four is that correct. it`s been a while um it looks like um but we`ve done all the physical checking we can we`re not we`ve got a one gig full duplex connection the only thing we haven`t ever changed was the vlan i was wondering if we could do that we`re because we`re we`ve changed the card we`ve changed ports um it`s an apartment. take. complex. a look at let. so we. me take a look at a couple of things here we`ll see what`s going on. okay the customers saying that it just we they don`t lose a physical connection they just lose the the internet connection like the i if we`re in our world the green light goes blue and we even changed them out from their old c four thousand to a new uh c fiftyfive hundred near the quantum uh the modem or smartnit excuse me with pods. oh what is that well days i`m showing they have an i p. yeah they are but like. okay. i said the only thing we haven`t tried changing was the vlan because they said they don`t lose the physical connection they just lose the the they lose their i p address and it comes back could we pop because i know there`s four there`s only two customers on that card according to pull d slam could we possibly assign two ninetyfive which is on port one of that card. just a moment. i`m sorry it`s on okay. two ninety five is in use. really yeah. that`s what it`s saying in q red doesn`t necessarily mean it`s in use in a quantum could be martin`s but i have a worker. is. on two ninety five. it on the the vlan of two seventeen or two thousand. mhm. seventeen two ninetyfive okay. yep. could we possibly just change one eightythree to something. i`m. else. working on it here now it takes a little bit. okay. of time to change. okay. the key line. sure you can see a lot more than i can all we can really see is like pull the slim but we don`t doesn`t look like that port one is up and working right now. and the two ninety five is working for a different number than the one that was originally signed to port one i can`t just steal it from that worker so. sure sure yeah i don`t we don`t know i just go out like i got three options and one`s working two of them are working and one says it`s not but now yeah i don`t know i have it hey all right i think i can exactly yeah yeah it`s basically all the go yeah i`ll send it to you. oh let`s check uh i`m gonna have you restart the smartnet. the whole uh the whole. well. um. i`ve got i`ve got the v c i changed so i`m gonna have you restarted it`s not bringing back any results here yet. okay let me uh i`ll call the customer and see if they can power cycle that real quick. o k mhm. hey tess this is jerry with centurylink hey can you do me a favor and just unplug the power from the back of that little uh that smartnet the little thing that says quantum on on the table there on the on the t v um can you see is the is the white port that does it have a light on it from the back okay cool okay perfect can you just uh power cycle that for me just unplug the power for about two seconds and then plug it back in from the back of it yep just from the back of the little light the little black port is pulled out wait two seconds to push it back in i`m working with someone on my work phone try to call you on my personal phone okay it`ll take it a second that we`ll go through some lights and then i`m gonna i was kind of i`m at the office and i was kind of watching to make sure that they changed the the vlan it was one eighty three now it`s four oh one so. mhm. i`m curious to what and the tech support person i`m working with wanted me to uh power cyclist i i want to make sure they change since i can physically see a change on the programming and uh sometimes they can tell you that they`ve done it and they didn`t so uh but she did at the point so i`m. yeah. uh yeah it`ll take a second so the customers saying that the the the lights go blue on the smart net. did it go blue because it should i don`t see any reason for it not to be pinging you ruled out a physical issue at the house. i`ve been there three times and we`ve got a physical we got a one gig connection it`s not a house it`s an apartment but yeah yeah we`ve changed the done just about everything um i did see that it changed from one eightythree to four oh one but i`ll uh i`ll get another smartnet plugged in outside and see what that does so i guess yeah i guess we`ve done everything on your end um i`ll know when i plug a smartnet in right to the port and uh i`ll i`ll go from there. and bounced everything i`ve rebuilt this customer got the v c i changed and updated in records. okay let me uh i don`t know if you want to hang on it takes about five minutes to go you probably have other people that you got to talk to. i see ping. well you do now. mmhm. ok. it just stopped in. sometimes it once we che we`ve chaired it`s not we haven`t done much on quantum quantum`s usually just thing this works great but these poor folks we`ve tried everything like i said but haven`t changed the the v landing well if you see ping i think i got ping so i won`t uh i won`t take up any more of your time. right anything. i`ll. else. go check. i can help. with the. with. customer. today. well that`s it thank you for your help i appreciate it. you`re welcome thank you for calling programming. bye.

